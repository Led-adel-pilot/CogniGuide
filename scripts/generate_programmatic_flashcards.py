#!/usr/bin/env python3
"""Generate Programmatic Flashcard landing pages from a CSV via the OpenAI SDK.

This script reads a CSV of page definitions, prompts a Gemini-capable OpenAI model
for rich SEO copy, and emits TypeScript objects that satisfy the
``ProgrammaticFlashcardPage`` schema. Run it locally before committing the resulting
TypeScript so that every generated page can be statically rendered by Next.js.

Example usage::

    python scripts/generate_programmatic_flashcards.py \
        --input data/flashcard_pages.csv \
        --output lib/programmatic/generated/flashcardPages.ts \
        --model gemini-2.5-flash-lite \
        --temperature 2.0 \
        --reasoning-effort high

The input CSV must include, at minimum, the columns ``slug`` and
``target_keyword``. Every other column becomes context that is injected into the
LLM prompt so you can steer copy for different audiences, intents, CTAs, etc.
"""

from __future__ import annotations

import argparse
import csv
import json
import random
import re
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Mapping

from openai import APIError, OpenAI, RateLimitError

DEFAULT_MODEL = "gemini-flash-lite-latest"
DEFAULT_TEMPERATURE = 1.0
DEFAULT_CONCURRENCY = 15
OUTPUT_HEADER = """// This file is autogenerated by scripts/generate_programmatic_flashcards.py
// Do not edit by hand — update the CSV and rerun the generator.

import type { ProgrammaticFlashcardPage } from '@/lib/programmatic/flashcardPageSchema';

export const generatedFlashcardPages: ProgrammaticFlashcardPage[] = [
"""
OUTPUT_FOOTER = "];\n"

PLACEHOLDER_RELATED_LINKS = [
  {"label": "/", "href": "/"},
  {"label": "/flashcards", "href": "/flashcards"},
]

PROMPT_TEMPLATE = """
You will generate a complete landing page JSON for an AI flashcards generator app, suitable for static rendering.
Info on app: You upload your PDFs, DOCX, powerpoint, images, or you can write a prompt eg "make flashcards on X", and the AI generates flashcards with spaced-repetition scheduling, you can also select the exam date, share flashcards with other through a public link. The app is free to use and also has a paid plan with more generation credits and the ability to use a more advanced AI model. Do not hallucinate other features (e.g. the app does not automatically tag concepts, or allow editing cards at the momment). 

You will receive structured CSV data for one landing page. Follow these requirements precisely:

CRITICAL WRITING RULES (people-first + E-E-A-T):
1)  **Write "People-First" Content:** Your primary goal is to create helpful, reliable, and original content that satisfies user search intent. The copy must be in-depth and high-utility. Avoid thin, boilerplate, or repetitive text. Each page must be unique and valuable.
2)  **Demonstrate E-E-A-T (Experience, Expertise, Authoritativeness, Trust):**
    * **Tone:** Write in a confident, informative, and trustworthy tone, as if from an expert in educational technology.
    * **Experience:** Show, don't just tell. Include specific benefits, use-cases, or hypothetical results (e.g., "...helped students cut study time in half" or "...improves exam scores by 20%").
    * **Expertise:** Weave in semantically related keywords and concepts naturally (e.g., "spaced repetition," "learning algorithms," "smarter studying," "cognitive science") to build topical authority.
    * **Evidence & Claims Policy (no hard citations required):** You may state general, experience-based benefits without sources. **Do not** include precise numeric claims (percent improvements, time reductions), named studies, or quotes **unless** you can provide a clear, verifiable source URL to a reputable site. If unsure, rewrite conservatively and favor specific examples over statistics.


ON-PAGE SEO REQUIREMENTS:
3) Title: ≤60 characters, must contain the target keyword or closest variant. Make it benefit-led.
4) Meta description: 140–155 characters, must begin with the target keyword or closest variant and promise the outcome without hype.
5) H1 (hero.heading): must include the target keyword. H1 ≈ title but not identical.
6) Headings hierarchy: Use concise H2/H3s; avoid keyword stuffing. Every section must be meaningfully different.
7) Semantic coverage: Naturally weave related_terms and subject-specific subtopics (if given). Include spaced repetition, active recall, tagging, and study workflow concepts where relevant.
8) Internal linking guidance: Provide two distinct anchor text variations for this page and two succinct descriptions (≤120 characters each) that other editors can use when linking to it from elsewhere on the site.
9) Accessibility: Any example references should describe content plainly so screen readers convey value (no images required in output).

CONVERSION (CTAs):
10) Use action-oriented, consistent CTAs (3-4 words max). Pick ONE primary label and reuse it consistently across the page. The CTAs in question will open the sign up for free account page.

EMBEDDED FLASHCARDS PREVIEW:
11) Craft exactly three topic-specific flashcards that would be what the user wanted if he searched for the target keyword with the intent of finding ready-made Flashcards.
    - The ready-made flashcard samples should be on the subject/topic of the target keyword.
    - If the target keyword is not a learning/studying topic, then come up with a relevant topic to the assumed audience that searches for such a keyword (assume high-school/college students).
    - Questions must be open-ended and atomic (less than 80 characters).
    - Answers must be concise (≤2 sentences) and accurate (less than 120 characters).
    - Mirror the language of the page (e.g., same locale, terminology).

STRUCTURED DATA:
12) Include FAQPage JSON-LD that mirrors the FAQ items. Breadcrumb trails are generated automatically, so do not add them.

OUTPUT FORMAT (STRICT):
Return ONLY a single valid JSON object with this shape (no markdown, no commentary). The slug and path are managed externally, so do not include them:

{
  "metadata": {
    "title": string,            // ≤60 chars, includes target keyword
    "description": string,      // 140–155 chars
    "keywords": string[],       // 5–10 semantic variants (for internal use; not meta keywords)
    "canonical": string         // base_url + "/flashcards/" + slug if not provided
  },
  "hero": {
    "heading": string,          // H1, contains target keyword/variant
    "subheading": string,       // 1–2 sentences: Explains what the tool does and how it could be helpful for someone searching for that target keyword (essentially Upload X or ask for X using a prompt and get flashcards, in contexts where asking the AI (prompting) is more suited start mentioning that, in contexts when uploading study materials is more suited mention that mainly).mentioning    "primaryCta": { "type": "modal", "label": string }
  },
  "featuresSection": {
    "heading": string,          // H2
    "subheading": string,
    "features": [{ "title": string, "description": string }, ...] // At least 3 features
  },
  "howItWorksSection": {
    "heading": string,          // H2
    "subheading": string,
    "steps": [{ "title": string, "description": string }, ...], // Exactly 3 steps
    "cta": { "type": "modal", "label": string }
  },
  "seoSection": {
    "heading": string,          // H2 (e.g., “Study Better with AI Flashcards”)
    "body": [
      { "type": "paragraph", "html": string }, // 180 words, contains target keyword/variant at the beginning.
      { "type": "list", "items": string[] },   // semantic/long-tail variants or use-cases
      { "type": "paragraph", "html": string }  // examples tailored to topic/subtopics
    ]
  },
  "faqSection": {
    "heading": string,          // H2
    "subheading": string,
    "items": [{ "question": string, "answer": string }, ...], // 4 distinct, relevant questions
    "cta": { "type": "modal", "label": string }
  },
  "linkingRecommendations": {
    "anchorTextVariants": [string, string],    // exactly 2 anchor texts variants, both variants must contain the target keyword/variant.
    "descriptionVariants": [string, string]    // exactly 2 short descriptions of the page you made, both variants must contain the target keyword/variant.
  },
  "embeddedFlashcards": [
    { "question": string, "answer": string },
    { "question": string, "answer": string },
    { "question": string, "answer": string }
  ],
  "structuredData": {
    "@context": "https://schema.org",
    "@graph": [
      {
        "@type": "FAQPage",
        "mainEntity": [
          {
            "@type": "Question",
            "name": "<FAQ 1 question>",
            "acceptedAnswer": {"@type":"Answer","text":"<FAQ 1 answer>"}
          }
        ]
      }
    ]
  }
}

QUALITY GATES (the model must self-check BEFORE returning JSON):
- Keep title ≤60 chars; description 140–155.
- H1 contains the target keyword or the closest natural variant.
- Ensure all HTML strings are safe for JSX.
- No placeholder text; no lorem ipsum. All fields must be complete and production-ready.
- Embedded flashcards must:
  * Ask open-ended, atomic questions tailored to the topic.
  * Provide concise answers (≤2 sentences) that support active recall.
  * Avoid markdown unless needed for short lists.
- Linking recommendations must contain exactly two items in each array and stay focused on this page's value proposition.

Return ONLY the JSON object. Ensure the first sentence of seoSection.body[0].html begins with the target keyword or its closest natural variant.
"""


SCRIPT_DIR = Path(__file__).resolve().parent
REPO_ROOT = SCRIPT_DIR.parent
TAXONOMY_PATH = REPO_ROOT / "data" / "flashcard_taxonomy.json"

TaxonomyEntry = Dict[str, str]
_TAXONOMY_CACHE: Dict[str, TaxonomyEntry] | None = None


def slugify(value: str) -> str:
  normalized = value.lower().replace("&", "and")
  normalized = re.sub(r"[^a-z0-9]+", "-", normalized)
  return normalized.strip("-")


def load_taxonomy_map() -> Dict[str, TaxonomyEntry]:
  global _TAXONOMY_CACHE
  if _TAXONOMY_CACHE is not None:
    return _TAXONOMY_CACHE

  try:
    raw = json.loads(TAXONOMY_PATH.read_text(encoding="utf-8"))
  except FileNotFoundError:
    _TAXONOMY_CACHE = {}
    return _TAXONOMY_CACHE

  mapping: Dict[str, TaxonomyEntry] = {}
  if isinstance(raw, dict):
    for hub_name, subhub_map in raw.items():
      if not isinstance(subhub_map, dict):
        continue
      hub_slug = slugify(str(hub_name))
      for subhub_name, slugs in subhub_map.items():
        if not isinstance(slugs, list):
          continue
        subhub_slug = slugify(str(subhub_name))
        for slug in slugs:
          if not isinstance(slug, str):
            continue
          mapping.setdefault(
            slug,
            {
              "hub_name": str(hub_name),
              "hub_slug": hub_slug,
              "subhub_name": str(subhub_name),
              "subhub_slug": subhub_slug,
            },
          )

  _TAXONOMY_CACHE = mapping
  return _TAXONOMY_CACHE


def build_breadcrumb_segments(row: CsvRow, breadcrumb_name: str) -> List[Dict[str, str]]:
  base_url = row.base_url.rstrip("/")
  segments: List[Dict[str, str]] = [
    {"name": "Home", "item": f"{base_url}/"},
    {"name": "Flashcards", "item": f"{base_url}/flashcards/"},
  ]

  taxonomy = load_taxonomy_map()
  entry = taxonomy.get(row.slug)
  if isinstance(entry, dict):
    hub_name = entry.get("hub_name")
    hub_slug = entry.get("hub_slug")
    if hub_name and hub_slug:
      segments.append(
        {
          "name": hub_name,
          "item": f"{base_url}/flashcards/{hub_slug}/",
        }
      )

    subhub_name = entry.get("subhub_name")
    subhub_slug = entry.get("subhub_slug")
    if subhub_name and subhub_slug and hub_slug:
      segments.append(
        {
          "name": subhub_name,
          "item": f"{base_url}/flashcards/{hub_slug}/{subhub_slug}/",
        }
      )

  breadcrumb_label = breadcrumb_name.strip() or row.slug
  segments.append({"name": breadcrumb_label})
  return segments


def parse_args(argv: Iterable[str] | None = None) -> argparse.Namespace:
  parser = argparse.ArgumentParser(description="Generate programmatic flashcard pages")
  parser.add_argument("--input", required=True, help="Path to the CSV containing page definitions")
  parser.add_argument(
    "--output",
    default="lib/programmatic/generated/flashcardPages.ts",
    help="Destination TypeScript file to overwrite",
  )
  parser.add_argument(
    "--model",
    default=DEFAULT_MODEL,
    help="OpenAI model name (Gemini-compatible models supported via the OpenAI SDK)",
  )
  parser.add_argument(
    "--temperature",
    type=float,
    default=DEFAULT_TEMPERATURE,
    help="Sampling temperature for creative variation",
  )
  parser.add_argument(
    "--max-rows",
    type=int,
    default=None,
    help="Optional maximum number of rows to process (for testing)",
  )
  parser.add_argument(
    "--concurrency",
    type=int,
    default=DEFAULT_CONCURRENCY,
    help="Maximum number of concurrent Gemini API requests to make",
  )
  parser.add_argument(
    "--rerun-existing",
    action="store_true",
    help="Regenerate rows whose slugs already exist in the output file",
  )
  parser.add_argument(
    "--api-key",
    default=None,
    help="Explicit Gemini API key. Falls back to the GEMINI_API_KEY env var if omitted.",
  )
  parser.add_argument(
    "--reasoning-effort",
    choices=["low", "medium", "high", "none"],
    default="none",
    help="Reasoning effort for Gemini 2.5 models: low (1,024 tokens), medium (8,192 tokens), high (24,576 tokens), or none (disable thinking).",
  )
  return parser.parse_args(argv)


@dataclass
class CsvRow:
  slug: str
  data: Mapping[str, str]

  @property
  def path(self) -> str:
    return self.data.get("path") or f"/flashcards/{self.slug}"

  @property
  def base_url(self) -> str:
    return self.data.get("base_url") or "https://www.cogniguide.app"

  def prompt_payload(self) -> str:
    payload = {
      "slug": self.slug,
      "base_url": self.base_url,
      "context": {k: v for k, v in self.data.items() if k not in {"slug", "path", "base_url"}},
    }
    return json.dumps(payload, ensure_ascii=False, indent=2)


def read_csv_rows(path: Path, max_rows: int | None = None) -> List[CsvRow]:
  with path.open(newline="", encoding="utf-8") as handle:
    reader = csv.DictReader(handle)
    if "slug" not in reader.fieldnames:
      raise ValueError("Input CSV must include a 'slug' column")
    if "target_keyword" not in reader.fieldnames:
      raise ValueError("Input CSV must include a 'target_keyword' column")

    rows: List[CsvRow] = []
    for idx, raw in enumerate(reader):
      if max_rows is not None and idx >= max_rows:
        break
      slug = (raw.get("slug") or "").strip()
      if not slug:
        raise ValueError(f"Row {idx + 2} is missing a slug")
      if not (raw.get("target_keyword") or "").strip():
        raise ValueError(f"Row {idx + 2} is missing a target_keyword")
      rows.append(CsvRow(slug=slug, data={k: str(v or '').strip() for k, v in raw.items()}))
    return rows


def call_model(
  client: OpenAI,
  model: str,
  temperature: float,
  payload: str,
  reasoning_effort: str | None = None,
) -> Dict[str, Any]:
  # Prepare the request parameters
  request_params = {
    "model": model,
    "temperature": temperature,
    "messages": [
      {
        "role": "system",
        "content": PROMPT_TEMPLATE,
      },
      {
        "role": "user",
        "content": f"CSV row JSON:\n{payload}",
      },
    ],
    "response_format": {
      "type": "json_schema",
      "json_schema": {
        "name": "programmatic_flashcard_page",
        "schema": {
          "type": "object",
          "properties": {
            "metadata": {"type": "object"},
            "hero": {"type": "object"},
            "featuresSection": {"type": "object"},
            "howItWorksSection": {"type": "object"},
            "seoSection": {"type": "object"},
            "faqSection": {"type": "object"},
            "relatedTopicsSection": {"type": "object"},
            "linkingRecommendations": {
              "type": "object",
              "properties": {
                "anchorTextVariants": {
                  "type": "array",
                  "items": {"type": "string"},
                  "minItems": 2,
                  "maxItems": 2,
                },
                "descriptionVariants": {
                  "type": "array",
                  "items": {"type": "string"},
                  "minItems": 2,
                  "maxItems": 2,
                },
              },
              "required": ["anchorTextVariants", "descriptionVariants"],
              "additionalProperties": False,
            },
            "embeddedFlashcards": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "question": {"type": "string"},
                  "answer": {"type": "string"},
                },
                "required": ["question", "answer"],
                "additionalProperties": False,
              },
              "minItems": 3,
              "maxItems": 3,
            },
          },
          "required": [
            "metadata",
            "hero",
            "featuresSection",
            "howItWorksSection",
            "seoSection",
            "faqSection",
            "linkingRecommendations",
            "embeddedFlashcards",
          ],
          "additionalProperties": True,
        },
      },
    },
  }

  # Add reasoning_effort if provided and not "none"
  if reasoning_effort and reasoning_effort != "none":
    request_params["reasoning_effort"] = reasoning_effort

  response = client.chat.completions.create(**request_params)

  if not response.choices or not response.choices[0].message.content:
    raise RuntimeError("Model returned an empty response")

  return json.loads(response.choices[0].message.content)


def call_model_with_retries(
  client: OpenAI,
  model: str,
  temperature: float,
  payload: str,
  reasoning_effort: str | None = None,
  *,
  max_attempts: int = 6,
  initial_delay: float = 4.0,
) -> Dict[str, Any]:
  delay = initial_delay
  for attempt in range(1, max_attempts + 1):
    try:
      return call_model(client, model, temperature, payload, reasoning_effort)
    except RateLimitError:
      if attempt == max_attempts:
        raise
      sleep_for = delay * (1 + random.random())
      print(
        f"Rate limit encountered (attempt {attempt}/{max_attempts}). "
        f"Retrying in {sleep_for:.2f} seconds...",
        file=sys.stderr,
      )
      time.sleep(sleep_for)
      delay *= 2
    except APIError as exc:
      status = getattr(exc, "status", None)
      if status == 429 and attempt < max_attempts:
        sleep_for = delay * (1 + random.random())
        print(
          f"Gemini API returned status 429 (attempt {attempt}/{max_attempts}). "
          f"Retrying in {sleep_for:.2f} seconds...",
          file=sys.stderr,
        )
        time.sleep(sleep_for)
        delay *= 2
        continue
      raise


def build_structured_data(row: CsvRow, payload: Dict[str, Any]) -> Dict[str, Any] | None:
  """Return structured data for ``payload`` when the model omits it."""

  metadata = payload.get("metadata") if isinstance(payload, dict) else {}
  if not isinstance(metadata, dict):
    metadata = {}

  path = payload.get("path") if isinstance(payload, dict) else None
  if not isinstance(path, str) or not path:
    path = row.path

  canonical = metadata.get("canonical")
  if not isinstance(canonical, str) or not canonical:
    canonical = f"{row.base_url.rstrip('/')}{path}"

  hero = payload.get("hero") if isinstance(payload, dict) else {}
  hero_heading = None
  if isinstance(hero, dict):
    heading = hero.get("heading")
    if isinstance(heading, str) and heading.strip():
      hero_heading = heading.strip()

  breadcrumb_name = hero_heading or row.data.get("target_keyword") or row.slug

  faq_section = payload.get("faqSection") if isinstance(payload, dict) else {}
  faq_items = []
  if isinstance(faq_section, dict):
    raw_items = faq_section.get("items")
    if isinstance(raw_items, list):
      for item in raw_items:
        if not isinstance(item, dict):
          continue
        question = item.get("question")
        answer = item.get("answer")
        if isinstance(question, str) and isinstance(answer, str) and question.strip() and answer.strip():
          faq_items.append(
            {
              "@type": "Question",
              "name": question.strip(),
              "acceptedAnswer": {"@type": "Answer", "text": answer.strip()},
            }
          )

  breadcrumb_segments = build_breadcrumb_segments(row, breadcrumb_name)
  item_list: List[Dict[str, Any]] = []
  for idx, segment in enumerate(breadcrumb_segments, start=1):
    item: Dict[str, Any] = {
      "@type": "ListItem",
      "position": idx,
      "name": segment.get("name") or "",
    }
    href = segment.get("item")
    if href:
      item["item"] = href
    item_list.append(item)

  breadcrumb_graph = {
    "@type": "BreadcrumbList",
    "itemListElement": item_list,
  }

  graph = [breadcrumb_graph]

  if faq_items:
    graph.append(
      {
        "@type": "FAQPage",
        "@id": f"{canonical}#faq",
        "url": canonical,
        "inLanguage": "en",
        "name": metadata.get("title") or breadcrumb_name,
        "mainEntity": faq_items,
      }
    )

  if not graph:
    return None

  return {"@context": "https://schema.org", "@graph": graph}


def normalise_page(row: CsvRow, payload: Dict[str, Any]) -> Dict[str, Any]:
  canonical = payload.get("metadata", {}).get("canonical")
  if not canonical:
    canonical = f"{row.base_url.rstrip('/')}{row.path}"
    payload.setdefault("metadata", {})["canonical"] = canonical

  payload.setdefault("path", row.path)
  payload["slug"] = row.slug

  linking_recs = payload.get("linkingRecommendations")
  if not isinstance(linking_recs, dict):
    raise ValueError(
      f"Model response for slug '{row.slug}' is missing linkingRecommendations"
    )

  for key in ("anchorTextVariants", "descriptionVariants"):
    values = linking_recs.get(key)
    if not isinstance(values, list) or len(values) < 2:
      raise ValueError(
        f"linkingRecommendations.{key} must include two entries for slug '{row.slug}'"
      )
    cleaned = [str(item).strip() for item in values if isinstance(item, str)][:2]
    if len(cleaned) < 2:
      raise ValueError(
        f"linkingRecommendations.{key} contained invalid entries for slug '{row.slug}'"
      )
    linking_recs[key] = cleaned

  placeholder_section = {
    "heading": payload.get("relatedTopicsSection", {}).get("heading")
    or "Explore related topics",
    "links": [dict(link) for link in PLACEHOLDER_RELATED_LINKS],
  }
  payload["relatedTopicsSection"] = placeholder_section

  structured_data = build_structured_data(row, payload)
  if structured_data:
    payload["structuredData"] = structured_data
  else:
    payload.pop("structuredData", None)
  return payload


def serialize_pages(pages: List[Dict[str, Any]]) -> str:
  body = ",\n".join(
    "  " + json.dumps(page, ensure_ascii=False, indent=2)
    .replace("\n", "\n  ")
    .replace("\\u2019", "’")
    for page in pages
  )
  return f"{OUTPUT_HEADER}{body}\n{OUTPUT_FOOTER}"


def write_output_file(path: Path, pages: List[Dict[str, Any]]) -> None:
  """Atomically write the generated pages to ``path``."""

  serialized = serialize_pages(pages)
  path.parent.mkdir(parents=True, exist_ok=True)

  temp_path = path.with_suffix(path.suffix + ".tmp")

  max_retries = 3
  for attempt in range(max_retries):
    try:
      temp_path.write_text(serialized, encoding="utf-8")
      temp_path.replace(path)
      return
    except PermissionError as e:
      if attempt == max_retries - 1:
        # Last attempt failed, try alternative approach
        print(f"Permission denied writing to {path}. Trying alternative method...", file=sys.stderr)
        try:
          # Try writing directly to the file (may leave partial content if interrupted)
          path.write_text(serialized, encoding="utf-8")
          return
        except PermissionError:
          raise PermissionError(
            f"Cannot write to {path}. The file may be open in another application (like your editor). "
            f"Please close the file and try again. Original error: {e}"
          )
      else:
        print(f"Permission denied (attempt {attempt + 1}/{max_retries}). Retrying...", file=sys.stderr)
        time.sleep(0.5)


def load_existing_pages(path: Path) -> List[Dict[str, Any]]:
  if not path.exists():
    return []

  text = path.read_text(encoding="utf-8")
  # Find the opening "[" that starts the array literal assigned to
  # generatedFlashcardPages. The first "[" in the file belongs to the
  # ProgrammaticFlashcardPage[] type annotation, so we explicitly search for
  # the "[" that follows the equals sign in the export.
  array_start_match = re.search(
    r"generatedFlashcardPages[^=]*=\s*\[",
    text,
    flags=re.MULTILINE,
  )
  if not array_start_match:
    print(
      f"Warning: Could not locate generatedFlashcardPages array in {path}. Ignoring its contents.",
      file=sys.stderr,
    )
    return []

  start = array_start_match.end() - 1

  # The array literal is terminated by the final "]" in the "\n];" footer.
  end_index = text.rfind("];")
  if end_index == -1 or end_index < start:
    print(
      f"Warning: Could not locate JSON array terminator in existing file {path}. Ignoring its contents.",
      file=sys.stderr,
    )
    return []

  try:
    parsed = json.loads(text[start : end_index + 1])
  except json.JSONDecodeError as exc:
    print(
      f"Warning: Failed to parse existing pages from {path}: {exc}. Ignoring its contents.",
      file=sys.stderr,
    )
    return []

  if not isinstance(parsed, list):
    print(
      f"Warning: Expected a list of pages in {path}, found {type(parsed).__name__}. Ignoring its contents.",
      file=sys.stderr,
    )
    return []

  pages: List[Dict[str, Any]] = []
  for item in parsed:
    if isinstance(item, dict):
      pages.append(item)
  return pages


def main(argv: Iterable[str] | None = None) -> int:
  args = parse_args(argv)
  input_path = Path(args.input)
  output_path = Path(args.output)

  rows = read_csv_rows(input_path, max_rows=args.max_rows)
  if not rows:
    print("No rows found in input CSV", file=sys.stderr)
    return 1

  client = OpenAI(
    api_key=args.api_key or "GEMINI_API_KEY",  # Use placeholder as per custom instructions
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
  )
  existing_pages = load_existing_pages(output_path)
  generated_pages: List[Dict[str, Any]] = list(existing_pages)
  slug_to_index: Dict[str, int] = {}
  for idx, page in enumerate(existing_pages):
    slug = page.get("slug") if isinstance(page, dict) else None
    if isinstance(slug, str):
      slug_to_index[slug] = idx

  regenerated_count = 0
  writes_performed = 0
  failed_rows: List[str] = []

  rows_to_generate: List[CsvRow] = []
  for row in rows:
    if row.slug in slug_to_index and not args.rerun_existing:
      print(f"Skipping slug '{row.slug}' (already present in {output_path})")
      continue
    rows_to_generate.append(row)

  if rows_to_generate:
    with ThreadPoolExecutor(max_workers=max(1, args.concurrency)) as executor:
      future_to_row = {
        executor.submit(
          call_model_with_retries,
          client,
          args.model,
          args.temperature,
          row.prompt_payload(),
          args.reasoning_effort,
        ): row
        for row in rows_to_generate
      }

      for future in as_completed(future_to_row):
        row = future_to_row[future]
        try:
          payload = future.result()
        except Exception as exc:  # noqa: BLE001
          failed_rows.append(row.slug)
          print(
            f"Error generating slug '{row.slug}': {exc}",
            file=sys.stderr,
          )
          continue

        page = normalise_page(row, payload)

        if row.slug in slug_to_index:
          generated_pages[slug_to_index[row.slug]] = page
        else:
          slug_to_index[row.slug] = len(generated_pages)
          generated_pages.append(page)

        regenerated_count += 1

        write_output_file(output_path, generated_pages)
        writes_performed += 1

  if not output_path.exists() or not writes_performed:
    write_output_file(output_path, generated_pages)

  if failed_rows:
    print(
      "The following slugs failed to generate: " + ", ".join(sorted(failed_rows)),
      file=sys.stderr,
    )
    return 1

  print(
    f"Wrote {len(generated_pages)} programmatic pages to {output_path}"
    + (f" (regenerated {regenerated_count} rows)" if regenerated_count else "")
  )
  return 0


if __name__ == "__main__":
  raise SystemExit(main())
