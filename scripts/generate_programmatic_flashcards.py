#!/usr/bin/env python3
"""Generate Programmatic Flashcard landing pages from a CSV via the OpenAI SDK.

This script reads a CSV of page definitions, prompts a Gemini-capable OpenAI model
for rich SEO copy, and emits TypeScript objects that satisfy the
``ProgrammaticFlashcardPage`` schema. Run it locally before committing the resulting
TypeScript so that every generated page can be statically rendered by Next.js.

Example usage::

    python scripts/generate_programmatic_flashcards.py \
        --input data/flashcard_pages.csv \
        --output lib/programmatic/generated/flashcardPages.ts \
        --model gemini-2.5-flash-lite \
        --temperature 2.0 \
        --reasoning-effort high

The input CSV must include, at minimum, the columns ``slug`` and
``target_keyword``. Every other column becomes context that is injected into the
LLM prompt so you can steer copy for different audiences, intents, CTAs, etc.
"""

from __future__ import annotations

import argparse
import csv
import json
import random
import re
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Mapping

from openai import APIError, OpenAI, RateLimitError

DEFAULT_MODEL = "gemini-flash-lite-latest"
DEFAULT_TEMPERATURE = 1.0
DEFAULT_CONCURRENCY = 15
OUTPUT_HEADER = """// This file is autogenerated by scripts/generate_programmatic_flashcards.py
// Do not edit by hand — update the CSV and rerun the generator.

import type { ProgrammaticFlashcardPage } from '@/lib/programmatic/flashcardPageSchema';

export const generatedFlashcardPages: ProgrammaticFlashcardPage[] = [
"""
OUTPUT_FOOTER = "];\n"

PROMPT_TEMPLATE = """
You will generate a complete landing page JSON for an AI flashcards generator app, suitable for static rendering.
Info on app: You upload your PDFs, DOCX, powerpoint or images and generates flashcards with spaced-repetition scheduling, you can also select the exam date, share flashcards with other through a public link. The app is free to use and also has a paid plan with more generation credits and the ability to use a more advanced AI model. Do not hallucinate other features (e.g. the app does not automatically tag concepts, or allow editing cards at the momment). 

You will receive structured CSV data for one landing page. Follow these requirements precisely:

CRITICAL WRITING RULES (people-first + E-E-A-T):
1)  **Write "People-First" Content:** Your primary goal is to create helpful, reliable, and original content that satisfies user search intent. The copy must be in-depth and high-utility. Avoid thin, boilerplate, or repetitive text. Each page must be unique and valuable.
2)  **Demonstrate E-E-A-T (Experience, Expertise, Authoritativeness, Trust):**
    * **Tone:** Write in a confident, informative, and trustworthy tone, as if from an expert in educational technology.
    * **Experience:** Show, don't just tell. Include specific benefits, use-cases, or hypothetical results (e.g., "...helped students cut study time in half" or "...improves exam scores by 20%").
    * **Expertise:** Weave in semantically related keywords and concepts naturally (e.g., "spaced repetition," "learning algorithms," "smarter studying," "cognitive science") to build topical authority.
    * **Evidence & Claims Policy (no hard citations required):** You may state general, experience-based benefits without sources. **Do not** include precise numeric claims (percent improvements, time reductions), named studies, or quotes **unless** you can provide a clear, verifiable source URL to a reputable site. If unsure, rewrite conservatively and favor specific examples over statistics.


ON-PAGE SEO REQUIREMENTS:
3) Title: ≤60 characters, must contain the target keyword or closest variant. Make it benefit-led.
4) Meta description: 140–155 characters, must begin with the target keyword or closest variant and promise the outcome without hype.
5) H1 (hero.heading): must include the target keyword. H1 ≈ title but not identical.
6) Headings hierarchy: Use concise H2/H3s; avoid keyword stuffing. Every section must be meaningfully different.
7) Semantic coverage: Naturally weave related_terms and subject-specific subtopics (if given). Include spaced repetition, active recall, tagging, and study workflow concepts where relevant.
8) Internal links: Use internal_links if provided; otherwise create at least two sensible links under relatedTopicsSection that point to topical neighbors at {base_url}.
9) Accessibility: Any example references should describe content plainly so screen readers convey value (no images required in output).

CONVERSION (CTAs):
10) Use action-oriented, consistent CTAs (3-4 words max). Pick ONE primary label and reuse it consistently across the page.

EMBEDDED FLASHCARDS PREVIEW:
11) Craft exactly three topic-specific flashcards that would be what the user wanted if he searched for the target keyword with the intent of finding ready-made Flashcards, but if the target keyword is most likely not for a ready-made flashcard intent, then write flashcards upselling benefit of AI Flashcards vs manual making or benefits of active recall, spaced repetition, etc. Do not make flashcards related to memorizing features of the app (e.g Don't make flashcards asking about what are the file formats supported for uploading).
    - Questions must be open-ended and atomic (less than 80 characters).
    - Answers must be concise (≤2 sentences) and accurate (less than 120 characters).
    - Mirror the language of the page (e.g., same locale, terminology).

STRUCTURED DATA:
12) Include FAQPage JSON-LD that mirrors the FAQ items. Also add a BreadcrumbList for:
    - “/flashcards” → “/flashcards/{slug}” (use base_url + "/flashcards/" + slug). Do not invent deeper levels.

OUTPUT FORMAT (STRICT):
Return ONLY a single valid JSON object with this shape (no markdown, no commentary). The slug and path are managed externally, so do not include them:

{
  "metadata": {
    "title": string,            // ≤60 chars, includes target keyword
    "description": string,      // 140–155 chars
    "keywords": string[],       // 5–10 semantic variants (for internal use; not meta keywords)
    "canonical": string         // base_url + "/flashcards/" + slug if not provided
  },
  "hero": {
    "heading": string,          // H1, contains target keyword/variant
    "subheading": string,       // 1–2 sentences: Explains what the tool does and how it could be helpfull for someone searching for that target keyword.
    "primaryCta": { "type": "modal", "label": string }
  },
  "featuresSection": {
    "heading": string,          // H2
    "subheading": string,
    "features": [{ "title": string, "description": string }, ...] // At least 3 features
  },
  "howItWorksSection": {
    "heading": string,          // H2
    "subheading": string,
    "steps": [{ "title": string, "description": string }, ...], // Exactly 3 steps
    "cta": { "type": "modal", "label": string }
  },
  "seoSection": {
    "heading": string,          // H2 (e.g., “Study Better with AI Flashcards”)
    "body": [
      { "type": "paragraph", "html": string }, // 180 words, contains target keyword/variant at the beginning.
      { "type": "list", "items": string[] },   // semantic/long-tail variants or use-cases
      { "type": "paragraph", "html": string }  // examples tailored to topic/subtopics
    ]
  },
  "faqSection": {
    "heading": string,          // H2
    "subheading": string,
    "items": [{ "question": string, "answer": string }, ...], // 4 distinct, relevant questions
    "cta": { "type": "modal", "label": string }
  },
 "relatedTopicsSection": {
    "heading": string,          // H2
    "links": [{ "label": string, "href": string, "description": string }, ...] // At least 2 internal links
  },
  "embeddedFlashcards": [
    { "question": string, "answer": string },
    { "question": string, "answer": string },
    { "question": string, "answer": string }
  ],
  "structuredData": {
    "@context": "https://schema.org",
    "@graph": [
      {
        "@type": "BreadcrumbList",
        "itemListElement": [
          {"@type":"ListItem","position":1,"name":"Flashcards","item":"{base_url}/flashcards"},
          {"@type":"ListItem","position":2,"name": "{context.target_keyword || slug}", "item": "{base_url}/flashcards/{slug}"}
        ]
      },
      {
        "@type": "FAQPage",
        "mainEntity": [
          {
            "@type": "Question",
            "name": "<FAQ 1 question>",
            "acceptedAnswer": {"@type":"Answer","text":"<FAQ 1 answer>"}
          }
        ]
      }
    ]
  }
}

QUALITY GATES (the model must self-check BEFORE returning JSON):
- Keep title ≤60 chars; description 140–155.
- H1 contains the target keyword or the closest natural variant.
- Ensure all HTML strings are safe for JSX.
- No placeholder text; no lorem ipsum. All fields must be complete and production-ready.
- Embedded flashcards must:
  * Ask open-ended, atomic questions tailored to the topic.
  * Provide concise answers (≤2 sentences) that support active recall.
  * Avoid markdown unless needed for short lists.

Return ONLY the JSON object. Ensure the first sentence of seoSection.body[0].html begins with the target keyword or its closest natural variant.
"""


def parse_args(argv: Iterable[str] | None = None) -> argparse.Namespace:
  parser = argparse.ArgumentParser(description="Generate programmatic flashcard pages")
  parser.add_argument("--input", required=True, help="Path to the CSV containing page definitions")
  parser.add_argument(
    "--output",
    default="lib/programmatic/generated/flashcardPages.ts",
    help="Destination TypeScript file to overwrite",
  )
  parser.add_argument(
    "--model",
    default=DEFAULT_MODEL,
    help="OpenAI model name (Gemini-compatible models supported via the OpenAI SDK)",
  )
  parser.add_argument(
    "--temperature",
    type=float,
    default=DEFAULT_TEMPERATURE,
    help="Sampling temperature for creative variation",
  )
  parser.add_argument(
    "--max-rows",
    type=int,
    default=None,
    help="Optional maximum number of rows to process (for testing)",
  )
  parser.add_argument(
    "--concurrency",
    type=int,
    default=DEFAULT_CONCURRENCY,
    help="Maximum number of concurrent Gemini API requests to make",
  )
  parser.add_argument(
    "--rerun-existing",
    action="store_true",
    help="Regenerate rows whose slugs already exist in the output file",
  )
  parser.add_argument(
    "--api-key",
    default=None,
    help="Explicit Gemini API key. Falls back to the GEMINI_API_KEY env var if omitted.",
  )
  parser.add_argument(
    "--reasoning-effort",
    choices=["low", "medium", "high", "none"],
    default="none",
    help="Reasoning effort for Gemini 2.5 models: low (1,024 tokens), medium (8,192 tokens), high (24,576 tokens), or none (disable thinking).",
  )
  return parser.parse_args(argv)


@dataclass
class CsvRow:
  slug: str
  data: Mapping[str, str]

  @property
  def path(self) -> str:
    return self.data.get("path") or f"/flashcards/{self.slug}"

  @property
  def base_url(self) -> str:
    return self.data.get("base_url") or "https://www.cogniguide.app"

  def prompt_payload(self) -> str:
    payload = {
      "slug": self.slug,
      "base_url": self.base_url,
      "context": {k: v for k, v in self.data.items() if k not in {"slug", "path", "base_url"}},
    }
    return json.dumps(payload, ensure_ascii=False, indent=2)


def read_csv_rows(path: Path, max_rows: int | None = None) -> List[CsvRow]:
  with path.open(newline="", encoding="utf-8") as handle:
    reader = csv.DictReader(handle)
    if "slug" not in reader.fieldnames:
      raise ValueError("Input CSV must include a 'slug' column")
    if "target_keyword" not in reader.fieldnames:
      raise ValueError("Input CSV must include a 'target_keyword' column")

    rows: List[CsvRow] = []
    for idx, raw in enumerate(reader):
      if max_rows is not None and idx >= max_rows:
        break
      slug = (raw.get("slug") or "").strip()
      if not slug:
        raise ValueError(f"Row {idx + 2} is missing a slug")
      if not (raw.get("target_keyword") or "").strip():
        raise ValueError(f"Row {idx + 2} is missing a target_keyword")
      rows.append(CsvRow(slug=slug, data={k: str(v or '').strip() for k, v in raw.items()}))
    return rows


def call_model(
  client: OpenAI,
  model: str,
  temperature: float,
  payload: str,
  reasoning_effort: str | None = None,
) -> Dict[str, Any]:
  # Prepare the request parameters
  request_params = {
    "model": model,
    "temperature": temperature,
    "messages": [
      {
        "role": "system",
        "content": PROMPT_TEMPLATE,
      },
      {
        "role": "user",
        "content": f"CSV row JSON:\n{payload}",
      },
    ],
    "response_format": {
      "type": "json_schema",
      "json_schema": {
        "name": "programmatic_flashcard_page",
        "schema": {
          "type": "object",
          "properties": {
            "metadata": {"type": "object"},
            "hero": {"type": "object"},
            "featuresSection": {"type": "object"},
            "howItWorksSection": {"type": "object"},
            "seoSection": {"type": "object"},
            "faqSection": {"type": "object"},
            "relatedTopicsSection": {"type": "object"},
            "embeddedFlashcards": {
              "type": "array",
              "items": {
                "type": "object",
                "properties": {
                  "question": {"type": "string"},
                  "answer": {"type": "string"},
                },
                "required": ["question", "answer"],
                "additionalProperties": False,
              },
              "minItems": 3,
              "maxItems": 3,
            },
          },
          "required": [
            "metadata",
            "hero",
            "featuresSection",
            "howItWorksSection",
            "seoSection",
            "faqSection",
            "relatedTopicsSection",
            "embeddedFlashcards",
          ],
          "additionalProperties": True,
        },
      },
    },
  }

  # Add reasoning_effort if provided and not "none"
  if reasoning_effort and reasoning_effort != "none":
    request_params["reasoning_effort"] = reasoning_effort

  response = client.chat.completions.create(**request_params)

  if not response.choices or not response.choices[0].message.content:
    raise RuntimeError("Model returned an empty response")

  return json.loads(response.choices[0].message.content)


def call_model_with_retries(
  client: OpenAI,
  model: str,
  temperature: float,
  payload: str,
  reasoning_effort: str | None = None,
  *,
  max_attempts: int = 6,
  initial_delay: float = 4.0,
) -> Dict[str, Any]:
  delay = initial_delay
  for attempt in range(1, max_attempts + 1):
    try:
      return call_model(client, model, temperature, payload, reasoning_effort)
    except RateLimitError:
      if attempt == max_attempts:
        raise
      sleep_for = delay * (1 + random.random())
      print(
        f"Rate limit encountered (attempt {attempt}/{max_attempts}). "
        f"Retrying in {sleep_for:.2f} seconds...",
        file=sys.stderr,
      )
      time.sleep(sleep_for)
      delay *= 2
    except APIError as exc:
      status = getattr(exc, "status", None)
      if status == 429 and attempt < max_attempts:
        sleep_for = delay * (1 + random.random())
        print(
          f"Gemini API returned status 429 (attempt {attempt}/{max_attempts}). "
          f"Retrying in {sleep_for:.2f} seconds...",
          file=sys.stderr,
        )
        time.sleep(sleep_for)
        delay *= 2
        continue
      raise


def build_structured_data(row: CsvRow, payload: Dict[str, Any]) -> Dict[str, Any] | None:
  """Return structured data for ``payload`` when the model omits it."""

  metadata = payload.get("metadata") if isinstance(payload, dict) else {}
  if not isinstance(metadata, dict):
    metadata = {}

  path = payload.get("path") if isinstance(payload, dict) else None
  if not isinstance(path, str) or not path:
    path = row.path

  canonical = metadata.get("canonical")
  if not isinstance(canonical, str) or not canonical:
    canonical = f"{row.base_url.rstrip('/')}{path}"

  hero = payload.get("hero") if isinstance(payload, dict) else {}
  hero_heading = None
  if isinstance(hero, dict):
    heading = hero.get("heading")
    if isinstance(heading, str) and heading.strip():
      hero_heading = heading.strip()

  breadcrumb_name = hero_heading or row.data.get("target_keyword") or row.slug

  faq_section = payload.get("faqSection") if isinstance(payload, dict) else {}
  faq_items = []
  if isinstance(faq_section, dict):
    raw_items = faq_section.get("items")
    if isinstance(raw_items, list):
      for item in raw_items:
        if not isinstance(item, dict):
          continue
        question = item.get("question")
        answer = item.get("answer")
        if isinstance(question, str) and isinstance(answer, str) and question.strip() and answer.strip():
          faq_items.append(
            {
              "@type": "Question",
              "name": question.strip(),
              "acceptedAnswer": {"@type": "Answer", "text": answer.strip()},
            }
          )

  breadcrumb_graph = {
    "@type": "BreadcrumbList",
    "itemListElement": [
      {
        "@type": "ListItem",
        "position": 1,
        "name": "Flashcards",
        "item": f"{row.base_url.rstrip('/')}/flashcards",
      },
      {
        "@type": "ListItem",
        "position": 2,
        "name": breadcrumb_name,
        "item": canonical,
      },
    ],
  }

  graph = [breadcrumb_graph]

  if faq_items:
    graph.append(
      {
        "@type": "FAQPage",
        "@id": f"{canonical}#faq",
        "url": canonical,
        "inLanguage": "en",
        "name": metadata.get("title") or breadcrumb_name,
        "mainEntity": faq_items,
      }
    )

  if not graph:
    return None

  return {"@context": "https://schema.org", "@graph": graph}


def normalise_page(row: CsvRow, payload: Dict[str, Any]) -> Dict[str, Any]:
  canonical = payload.get("metadata", {}).get("canonical")
  if not canonical:
    canonical = f"{row.base_url.rstrip('/')}{row.path}"
    payload.setdefault("metadata", {})["canonical"] = canonical

  payload.setdefault("path", row.path)
  payload["slug"] = row.slug

  structured_data = payload.get("structuredData")
  if not isinstance(structured_data, dict) or not structured_data.get("@graph"):
    fallback_structured = build_structured_data(row, payload)
    if fallback_structured:
      payload["structuredData"] = fallback_structured
    else:
      payload.pop("structuredData", None)
  return payload


def serialize_pages(pages: List[Dict[str, Any]]) -> str:
  body = ",\n".join(
    "  " + json.dumps(page, ensure_ascii=False, indent=2)
    .replace("\n", "\n  ")
    .replace("\\u2019", "’")
    for page in pages
  )
  return f"{OUTPUT_HEADER}{body}\n{OUTPUT_FOOTER}"


def write_output_file(path: Path, pages: List[Dict[str, Any]]) -> None:
  """Atomically write the generated pages to ``path``."""

  serialized = serialize_pages(pages)
  path.parent.mkdir(parents=True, exist_ok=True)

  temp_path = path.with_suffix(path.suffix + ".tmp")

  max_retries = 3
  for attempt in range(max_retries):
    try:
      temp_path.write_text(serialized, encoding="utf-8")
      temp_path.replace(path)
      return
    except PermissionError as e:
      if attempt == max_retries - 1:
        # Last attempt failed, try alternative approach
        print(f"Permission denied writing to {path}. Trying alternative method...", file=sys.stderr)
        try:
          # Try writing directly to the file (may leave partial content if interrupted)
          path.write_text(serialized, encoding="utf-8")
          return
        except PermissionError:
          raise PermissionError(
            f"Cannot write to {path}. The file may be open in another application (like your editor). "
            f"Please close the file and try again. Original error: {e}"
          )
      else:
        print(f"Permission denied (attempt {attempt + 1}/{max_retries}). Retrying...", file=sys.stderr)
        time.sleep(0.5)


def load_existing_pages(path: Path) -> List[Dict[str, Any]]:
  if not path.exists():
    return []

  text = path.read_text(encoding="utf-8")
  # Find the opening "[" that starts the array literal assigned to
  # generatedFlashcardPages. The first "[" in the file belongs to the
  # ProgrammaticFlashcardPage[] type annotation, so we explicitly search for
  # the "[" that follows the equals sign in the export.
  array_start_match = re.search(
    r"generatedFlashcardPages[^=]*=\s*\[",
    text,
    flags=re.MULTILINE,
  )
  if not array_start_match:
    print(
      f"Warning: Could not locate generatedFlashcardPages array in {path}. Ignoring its contents.",
      file=sys.stderr,
    )
    return []

  start = array_start_match.end() - 1

  # The array literal is terminated by the final "]" in the "\n];" footer.
  end_index = text.rfind("];")
  if end_index == -1 or end_index < start:
    print(
      f"Warning: Could not locate JSON array terminator in existing file {path}. Ignoring its contents.",
      file=sys.stderr,
    )
    return []

  try:
    parsed = json.loads(text[start : end_index + 1])
  except json.JSONDecodeError as exc:
    print(
      f"Warning: Failed to parse existing pages from {path}: {exc}. Ignoring its contents.",
      file=sys.stderr,
    )
    return []

  if not isinstance(parsed, list):
    print(
      f"Warning: Expected a list of pages in {path}, found {type(parsed).__name__}. Ignoring its contents.",
      file=sys.stderr,
    )
    return []

  pages: List[Dict[str, Any]] = []
  for item in parsed:
    if isinstance(item, dict):
      pages.append(item)
  return pages


def main(argv: Iterable[str] | None = None) -> int:
  args = parse_args(argv)
  input_path = Path(args.input)
  output_path = Path(args.output)

  rows = read_csv_rows(input_path, max_rows=args.max_rows)
  if not rows:
    print("No rows found in input CSV", file=sys.stderr)
    return 1

  client = OpenAI(
    api_key=args.api_key or "GEMINI_API_KEY",  # Use placeholder as per custom instructions
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
  )
  existing_pages = load_existing_pages(output_path)
  generated_pages: List[Dict[str, Any]] = list(existing_pages)
  slug_to_index: Dict[str, int] = {}
  for idx, page in enumerate(existing_pages):
    slug = page.get("slug") if isinstance(page, dict) else None
    if isinstance(slug, str):
      slug_to_index[slug] = idx

  regenerated_count = 0
  writes_performed = 0
  failed_rows: List[str] = []

  rows_to_generate: List[CsvRow] = []
  for row in rows:
    if row.slug in slug_to_index and not args.rerun_existing:
      print(f"Skipping slug '{row.slug}' (already present in {output_path})")
      continue
    rows_to_generate.append(row)

  if rows_to_generate:
    with ThreadPoolExecutor(max_workers=max(1, args.concurrency)) as executor:
      future_to_row = {
        executor.submit(
          call_model_with_retries,
          client,
          args.model,
          args.temperature,
          row.prompt_payload(),
          args.reasoning_effort,
        ): row
        for row in rows_to_generate
      }

      for future in as_completed(future_to_row):
        row = future_to_row[future]
        try:
          payload = future.result()
        except Exception as exc:  # noqa: BLE001
          failed_rows.append(row.slug)
          print(
            f"Error generating slug '{row.slug}': {exc}",
            file=sys.stderr,
          )
          continue

        page = normalise_page(row, payload)

        if row.slug in slug_to_index:
          generated_pages[slug_to_index[row.slug]] = page
        else:
          slug_to_index[row.slug] = len(generated_pages)
          generated_pages.append(page)

        regenerated_count += 1

        write_output_file(output_path, generated_pages)
        writes_performed += 1

  if not output_path.exists() or not writes_performed:
    write_output_file(output_path, generated_pages)

  if failed_rows:
    print(
      "The following slugs failed to generate: " + ", ".join(sorted(failed_rows)),
      file=sys.stderr,
    )
    return 1

  print(
    f"Wrote {len(generated_pages)} programmatic pages to {output_path}"
    + (f" (regenerated {regenerated_count} rows)" if regenerated_count else "")
  )
  return 0


if __name__ == "__main__":
  raise SystemExit(main())
