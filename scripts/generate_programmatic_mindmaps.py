#!/usr/bin/env python3
"""Generate Programmatic Mind Map landing pages from a CSV via the OpenAI SDK.

This script reads a CSV of page definitions, prompts a Gemini-capable OpenAI model
for rich SEO copy, and emits TypeScript objects that satisfy the
``ProgrammaticMindMapPage`` schema. Run it locally before committing the resulting
TypeScript so that every generated page can be statically rendered by Next.js.

Example usage::

python scripts/generate_programmatic_mindmaps.py --input data/mindmap_pages.csv --output lib/programmatic/generated/mindMapPages.ts --model gemini-flash-lite-latest --max-api-calls 3 --concurrency 5

The input CSV must include, at minimum, the columns ``slug`` and
``target_keyword``. Every other column becomes context that is injected into the
LLM prompt so you can steer copy for different audiences, intents, CTAs, etc.
"""

from __future__ import annotations

import argparse
import copy
import csv
import json
import random
import re
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Mapping

from openai import APIError, OpenAI, RateLimitError

DEFAULT_MODEL = "gemini-flash-lite-latest"
DEFAULT_TEMPERATURE = 1.0
DEFAULT_CONCURRENCY = 15
OUTPUT_HEADER = """// @ts-nocheck
// This file is autogenerated by scripts/generate_programmatic_mindmaps.py
// Do not edit by hand — update the CSV and rerun the generator.

import type { ProgrammaticMindMapPage } from '@/lib/programmatic/mindMapPageSchema';

export const generatedMindMapPages: ProgrammaticMindMapPage[] = [
"""
OUTPUT_FOOTER = "];\n"

PLACEHOLDER_RELATED_LINKS = [
  {"label": "/", "href": "/"},
  {"label": "/mind-maps", "href": "/mind-maps"},
]

PROMPT_TEMPLATE = """
You will generate a complete landing page JSON for CogniGuide’s AI mind map generator app, suitable for static rendering.
Info on app: Users upload PDFs, DOCX, PPTX, text, or prompt the AI to build interactive mind maps. CogniGuide restructures the content into expandable branches, supports custom colors, exports (SVG/PNG/PDF), collaboration via share links, and lets users convert branches into flashcards. The core experience is instant, visual mind mapping—do not claim unrelated features (no Kanban boards, auto task tracking, etc.).

You will receive structured CSV data for one landing page. Follow these requirements precisely:

CRITICAL WRITING RULES (people-first + E-E-A-T):
1)  **Write "People-First" Content:** Prioritize helpful, reliable, original explanations that satisfy the searcher’s mind map intent. Blend storytelling with concrete workflow examples so each page feels handcrafted.
2)  **Demonstrate E-E-A-T:**
    * **Tone:** Confident, instructive, and encouraging—speak as a product marketer who deeply understands visual thinking.
    * **Experience:** Reference specific mind map workflows (study outlines, SOP mapping, meeting debriefs, research synthesis) instead of generic language.
    * **Expertise:** Naturally weave semantic phrases like “diagram complex systems,” “concept mapping,” “hierarchical structure,” “brainstorm visibility,” “visual knowledge base.”
    * **Evidence & Claims:** Use qualitative proof (“teams unblock workshops faster”) instead of unsupported numbers/studies.

PSYCHOLOGY-DRIVEN CTR GUIDANCE FOR METADATA:
- Curiosity gap: Promise a transformation (“turn chaotic research into visual clarity”) while staying truthful.
- Loss aversion: Highlight pain avoided by using CogniGuide (“don’t spend Sunday redrawing mind maps”).
- Emotional resonance: Mirror the target audience’s stress (“overwhelmed product brief,” “exam cram chaos”) in the meta description.

3) Title requirements:
- <=60 characters.
- Include the target keyword or its natural variant in the first half.
- Communicate the exact benefit (“AI mind map maker for nursing students,” etc.).

4) Meta description (120–155 chars):
- Mention the target keyword once.
- Describe the real workflow (upload/paste → AI generates mind maps you can edit/export).
- Include two benefits max (e.g., faster clarity, easier collaboration).
- End with a soft CTA (“Start mapping free”).

5) On-page copy:
- Hero heading/subheading must clarify the use-case and point to visual outcomes (clarity, alignment, studying).
- Features section: 3 benefits that speak to inputs (files, prompts), outputs (structure, exports), and downstream actions (teaching, decision-making).
- How it works: Exactly 3 steps tuned to the CSV row’s context. Emphasize visual transformation, editing, and next action (export/share/flashcards).
- SEO section: Body paragraphs + list should reference adjacent keywords (concept maps, idea maps, brainstorming, curriculum planning, etc.).
- FAQ: 4 topic-specific Q&As. Show empathy for blockers (format support, collaboration, exporting, reliability).

6) Embedded mind map preview:
- Provide a Markdown string in Markmap syntax that mirrors what a user searching the target keyword expects to visualize.
- Keep hierarchy 3 levels deep max; no lorem ipsum. Emphasize subtopics and supporting bullets.
- Include a `title` and `description` summarizing what the preview shows.

7) Structured data:
- Provide breadcrumb + FAQ JSON-LD if omitted; your JSON output may include it directly, but it must follow schema.org formats.

Return JSON with this shape (comments describe expectations):

```json
{
  "metadata": {
    "title": "...",
    "description": "...",
    "keywords": ["mind map keyword variants", "..."],
    "canonical": "https://www.cogniguide.app/mind-maps/{slug}"
  },
  "hero": {
    "eyebrow": string,
    "heading": string,        // contains the target keyword or closest variant
    "subheading": string,
    "primaryCta": { "type": "modal", "label": "Generate mind map" }
  },
  "featuresSection": {
    "heading": string,
    "subheading": string,
    "features": [{ "title": string, "description": string }, ...] // 3–4 cards
  },
  "howItWorksSection": {
    "heading": string,
    "subheading": string,
    "steps": [{ "title": string, "description": string }, ...], // exactly 3
    "cta": { "type": "link", "label": string, "href": "/pricing" }
  },
  "seoSection": {
    "heading": string,
    "body": [
      { "type": "paragraph", "html": string }, // first sentence starts with the keyword or close variant
      { "type": "list", "items": [string, ...] },
      { "type": "paragraph", "html": string }
    ]
  },
  "faqSection": {
    "heading": string,
    "subheading": string,
    "items": [{ "question": string, "answer": string }, ...], // 4 items
    "cta": { "type": "modal", "label": string }
  },
  "linkingRecommendations": {
    "anchorText": string, // 3–5 words, must include the keyword variant
    "descriptionVariants": [string, string]
  },
  "embeddedMindMap": {
    "title": string,
    "description": string,
    "markdown": string // Valid Markmap markdown preview tailored to the keyword
  },
  "relatedTopicsSection": {
    "heading": string,
    "links": [{ "label": string, "href": string, "description": string }, ...]
  }
}
```

QUALITY GATES:
- Never return placeholders. Every string must be production-ready.
- Avoid duplication; each page must feel unique to its keyword/audience.
- Keep copy grounded in real app functionality (upload, prompt, edit, export, convert to flashcards).
- Embedded mind map markdown must be safe for HTML (no raw script tags) and use bullet hierarchy only.
- linkingRecommendations.descriptionVariants must each mention the target keyword or a natural derivative.
"""

REASONING_EFFORT_BUDGETS = {
  "low": 512,
  "medium": 1024,
  "high": 8192,
}


SCRIPT_DIR = Path(__file__).resolve().parent
REPO_ROOT = SCRIPT_DIR.parent
TAXONOMY_PATH = REPO_ROOT / "data" / "mindmap_taxonomy.json"

TaxonomyEntry = Dict[str, str]
_TAXONOMY_CACHE: Dict[str, TaxonomyEntry] | None = None


def slugify(value: str) -> str:
  normalized = value.lower().replace("&", "and")
  normalized = re.sub(r"[^a-z0-9]+", "-", normalized)
  return normalized.strip("-")


def load_taxonomy_map() -> Dict[str, TaxonomyEntry]:
  global _TAXONOMY_CACHE
  if _TAXONOMY_CACHE is not None:
    return _TAXONOMY_CACHE

  try:
    raw = json.loads(TAXONOMY_PATH.read_text(encoding="utf-8"))
  except FileNotFoundError:
    _TAXONOMY_CACHE = {}
    return _TAXONOMY_CACHE

  mapping: Dict[str, TaxonomyEntry] = {}
  if isinstance(raw, dict):
    for hub_name, subhub_map in raw.items():
      if not isinstance(subhub_map, dict):
        continue
      hub_slug = slugify(str(hub_name))
      for subhub_name, slugs in subhub_map.items():
        if not isinstance(slugs, list):
          continue
        subhub_slug = slugify(str(subhub_name))
        for slug in slugs:
          if not isinstance(slug, str):
            continue
          mapping.setdefault(
            slug,
            {
              "hub_name": str(hub_name),
              "hub_slug": hub_slug,
              "subhub_name": str(subhub_name),
              "subhub_slug": subhub_slug,
            },
          )

  _TAXONOMY_CACHE = mapping
  return _TAXONOMY_CACHE


def build_breadcrumb_segments(row: CsvRow, breadcrumb_name: str) -> List[Dict[str, str]]:
  base_url = row.base_url.rstrip("/")
  segments: List[Dict[str, str]] = [
    {"name": "Home", "item": f"{base_url}/"},
    {"name": "Mind Maps", "item": f"{base_url}/mind-maps/"},
  ]

  taxonomy = load_taxonomy_map()
  entry = taxonomy.get(row.slug)
  if isinstance(entry, dict):
    hub_name = entry.get("hub_name")
    hub_slug = entry.get("hub_slug")
    if hub_name and hub_slug:
      segments.append(
        {
          "name": hub_name,
          "item": f"{base_url}/mind-maps/{hub_slug}/",
        }
      )

    subhub_name = entry.get("subhub_name")
    subhub_slug = entry.get("subhub_slug")
    if subhub_name and subhub_slug and hub_slug:
      segments.append(
        {
          "name": subhub_name,
          "item": f"{base_url}/mind-maps/{hub_slug}/{subhub_slug}/",
        }
      )

  breadcrumb_label = breadcrumb_name.strip() or row.slug
  segments.append({"name": breadcrumb_label})
  return segments


def parse_args(argv: Iterable[str] | None = None) -> argparse.Namespace:
  parser = argparse.ArgumentParser(description="Generate programmatic mind map pages")
  parser.add_argument("--input", required=True, help="Path to the CSV containing page definitions")
  parser.add_argument(
    "--output",
    default="lib/programmatic/generated/mindMapPages.ts",
    help="Destination TypeScript file to overwrite",
  )
  parser.add_argument(
    "--model",
    default=DEFAULT_MODEL,
    help="OpenAI model name (Gemini-compatible models supported via the OpenAI SDK)",
  )
  parser.add_argument(
    "--temperature",
    type=float,
    default=DEFAULT_TEMPERATURE,
    help="Sampling temperature for creative variation",
  )
  parser.add_argument(
    "--max-api-calls",
    type=int,
    default=500,
    help="Optional maximum number of Gemini API calls to make (for testing)",
  )
  parser.add_argument(
    "--concurrency",
    type=int,
    default=DEFAULT_CONCURRENCY,
    help="Maximum number of concurrent Gemini API requests to make",
  )
  parser.add_argument(
    "--rerun-existing",
    action="store_true",
    help="Regenerate rows whose slugs already exist in the output file",
  )
  parser.add_argument(
    "--api-key",
    default=None,
    help="Explicit Gemini API key. Falls back to the GEMINI_API_KEY env var if omitted.",
  )
  parser.add_argument(
    "--reasoning-effort",
    choices=["low", "medium", "high", "none"],
    default="low",
    help="Reasoning effort for Gemini 2.5 models: low (1,024 tokens), medium (8,192 tokens), high (24,576 tokens), or none (disable thinking).",
  )
  return parser.parse_args(argv)


@dataclass
class CsvRow:
  slug: str
  data: Mapping[str, str]

  @property
  def path(self) -> str:
    return self.data.get("path") or f"/mind-maps/{self.slug}"

  @property
  def base_url(self) -> str:
    return self.data.get("base_url") or "https://www.cogniguide.app"

  def prompt_payload(self) -> str:
    payload = {
      "slug": self.slug,
      "base_url": self.base_url,
      "context": {k: v for k, v in self.data.items() if k not in {"slug", "path", "base_url"}},
    }
    return json.dumps(payload, ensure_ascii=False, indent=2)


def read_csv_rows(path: Path) -> List[CsvRow]:
  with path.open(newline="", encoding="utf-8") as handle:
    reader = csv.DictReader(handle)
    if "slug" not in reader.fieldnames:
      raise ValueError("Input CSV must include a 'slug' column")
    if "target_keyword" not in reader.fieldnames:
      raise ValueError("Input CSV must include a 'target_keyword' column")

    rows: List[CsvRow] = []
    for idx, raw in enumerate(reader):
      slug = (raw.get("slug") or "").strip()
      if not slug:
        raise ValueError(f"Row {idx + 2} is missing a slug")
      if not (raw.get("target_keyword") or "").strip():
        raise ValueError(f"Row {idx + 2} is missing a target_keyword")
      rows.append(CsvRow(slug=slug, data={k: str(v or '').strip() for k, v in raw.items()}))
    return rows


def _inject_thinking_budget(
  request_params: Dict[str, Any],
  reasoning_effort: str,
) -> Dict[str, Any]:
  """Fallback for SDKs that do not expose reasoning_effort."""

  budget = REASONING_EFFORT_BUDGETS.get(reasoning_effort)
  if not budget:
    return request_params

  extra_body = copy.deepcopy(request_params.get("extra_body") or {})
  nested_extra = extra_body.setdefault("extra_body", {})
  google_section = nested_extra.setdefault("google", {})
  thinking_config = google_section.setdefault("thinking_config", {})
  thinking_config.setdefault("thinking_budget", budget)
  google_section["thinking_config"] = thinking_config
  request_params["extra_body"] = extra_body
  return request_params


def call_model(
  client: OpenAI,
  model: str,
  temperature: float,
  payload: str,
  reasoning_effort: str | None = None,
) -> Dict[str, Any]:
  # Prepare the request parameters
  request_params = {
    "model": model,
    "temperature": temperature,
    "messages": [
      {
        "role": "system",
        "content": PROMPT_TEMPLATE,
      },
      {
        "role": "user",
        "content": f"CSV row JSON:\n{payload}",
      },
    ],
    "response_format": {
      "type": "json_schema",
      "json_schema": {
        "name": "programmatic_mindmap_page",
        "schema": {
          "type": "object",
          "properties": {
            "metadata": {"type": "object"},
            "hero": {"type": "object"},
            "featuresSection": {"type": "object"},
            "howItWorksSection": {"type": "object"},
            "seoSection": {"type": "object"},
            "faqSection": {"type": "object"},
            "relatedTopicsSection": {"type": "object"},
            "linkingRecommendations": {
              "type": "object",
              "properties": {
                "anchorText": {"type": "string"},
                "descriptionVariants": {
                  "type": "array",
                  "items": {"type": "string"},
                  "minItems": 2,
                  "maxItems": 2,
                },
              },
              "required": ["anchorText", "descriptionVariants"],
              "additionalProperties": False,
            },
            "embeddedMindMap": {
              "type": "object",
              "properties": {
                "title": {"type": "string"},
                "description": {"type": "string"},
                "markdown": {"type": "string"},
              },
              "required": ["markdown"],
              "additionalProperties": False,
            },
          },
          "required": [
            "metadata",
            "hero",
            "featuresSection",
            "howItWorksSection",
            "seoSection",
            "faqSection",
            "linkingRecommendations",
            "embeddedMindMap",
          ],
          "additionalProperties": True,
        },
      },
    },
  }

  # Add reasoning_effort if provided and not "none"
  use_reasoning = reasoning_effort and reasoning_effort != "none"
  if use_reasoning:
    request_params["reasoning_effort"] = reasoning_effort

  try:
    response = client.chat.completions.create(**request_params)
  except TypeError as exc:
    if not use_reasoning or "reasoning_effort" not in str(exc):
      raise

    # Some older OpenAI SDK builds reject reasoning_effort; fall back to thinking budget.
    assert reasoning_effort is not None
    fallback_params = copy.deepcopy(request_params)
    fallback_params.pop("reasoning_effort", None)
    fallback_params = _inject_thinking_budget(fallback_params, reasoning_effort)
    response = client.chat.completions.create(**fallback_params)

  if not response.choices or not response.choices[0].message.content:
    raise RuntimeError("Model returned an empty response")

  return json.loads(response.choices[0].message.content)


def call_model_with_retries(
  client: OpenAI,
  model: str,
  temperature: float,
  payload: str,
  reasoning_effort: str | None = None,
  *,
  max_attempts: int = 6,
  initial_delay: float = 4.0,
) -> Dict[str, Any]:
  delay = initial_delay
  for attempt in range(1, max_attempts + 1):
    try:
      return call_model(client, model, temperature, payload, reasoning_effort)
    except RateLimitError:
      if attempt == max_attempts:
        raise
      sleep_for = delay * (1 + random.random())
      print(
        f"Rate limit encountered (attempt {attempt}/{max_attempts}). "
        f"Retrying in {sleep_for:.2f} seconds...",
        file=sys.stderr,
      )
      time.sleep(sleep_for)
      delay *= 2
    except APIError as exc:
      status = getattr(exc, "status", None)
      if status == 429 and attempt < max_attempts:
        sleep_for = delay * (1 + random.random())
        print(
          f"Gemini API returned status 429 (attempt {attempt}/{max_attempts}). "
          f"Retrying in {sleep_for:.2f} seconds...",
          file=sys.stderr,
        )
        time.sleep(sleep_for)
        delay *= 2
        continue
      raise


def build_structured_data(row: CsvRow, payload: Dict[str, Any]) -> Dict[str, Any] | None:
  """Return structured data for ``payload`` when the model omits it."""

  metadata = payload.get("metadata") if isinstance(payload, dict) else {}
  if not isinstance(metadata, dict):
    metadata = {}

  path = payload.get("path") if isinstance(payload, dict) else None
  if not isinstance(path, str) or not path:
    path = row.path

  canonical = metadata.get("canonical")
  if not isinstance(canonical, str) or not canonical:
    canonical = f"{row.base_url.rstrip('/')}{path}"

  hero = payload.get("hero") if isinstance(payload, dict) else {}
  hero_heading = None
  if isinstance(hero, dict):
    heading = hero.get("heading")
    if isinstance(heading, str) and heading.strip():
      hero_heading = heading.strip()

  breadcrumb_name = hero_heading or row.data.get("target_keyword") or row.slug

  faq_section = payload.get("faqSection") if isinstance(payload, dict) else {}
  faq_items = []
  if isinstance(faq_section, dict):
    raw_items = faq_section.get("items")
    if isinstance(raw_items, list):
      for item in raw_items:
        if not isinstance(item, dict):
          continue
        question = item.get("question")
        answer = item.get("answer")
        if isinstance(question, str) and isinstance(answer, str) and question.strip() and answer.strip():
          faq_items.append(
            {
              "@type": "Question",
              "name": question.strip(),
              "acceptedAnswer": {"@type": "Answer", "text": answer.strip()},
            }
          )

  breadcrumb_segments = build_breadcrumb_segments(row, breadcrumb_name)
  item_list: List[Dict[str, Any]] = []
  for idx, segment in enumerate(breadcrumb_segments, start=1):
    item: Dict[str, Any] = {
      "@type": "ListItem",
      "position": idx,
      "name": segment.get("name") or "",
    }
    href = segment.get("item")
    if href:
      item["item"] = href
    item_list.append(item)

  breadcrumb_graph = {
    "@type": "BreadcrumbList",
    "itemListElement": item_list,
  }

  graph = [breadcrumb_graph]

  if faq_items:
    graph.append(
      {
        "@type": "FAQPage",
        "@id": f"{canonical}#faq",
        "url": canonical,
        "inLanguage": "en",
        "name": metadata.get("title") or breadcrumb_name,
        "mainEntity": faq_items,
      }
    )

  if not graph:
    return None

  return {"@context": "https://schema.org", "@graph": graph}


def normalise_page(row: CsvRow, payload: Dict[str, Any]) -> Dict[str, Any]:
  metadata = payload.get("metadata")
  if not isinstance(metadata, dict):
    raise ValueError(f"Model response for slug '{row.slug}' is missing metadata")

  metadata["canonical"] = f"{row.base_url.rstrip('/')}{row.path}"

  payload["path"] = row.path
  payload["slug"] = row.slug

  linking_recs = payload.get("linkingRecommendations")
  if not isinstance(linking_recs, dict):
    raise ValueError(
      f"Model response for slug '{row.slug}' is missing linkingRecommendations"
    )

  anchor_text = linking_recs.get("anchorText")
  if not isinstance(anchor_text, str) or not anchor_text.strip():
    raise ValueError(
      f"linkingRecommendations.anchorText must be a non-empty string for slug '{row.slug}'"
    )
  linking_recs["anchorText"] = anchor_text.strip()

  description_variants = linking_recs.get("descriptionVariants")
  if not isinstance(description_variants, list) or len(description_variants) < 2:
    raise ValueError(
      f"linkingRecommendations.descriptionVariants must include two entries for slug '{row.slug}'"
    )
  cleaned_descriptions = [
    str(item).strip() for item in description_variants if isinstance(item, str)
  ][:2]
  if len(cleaned_descriptions) < 2:
    raise ValueError(
      f"linkingRecommendations.descriptionVariants contained invalid entries for slug '{row.slug}'"
    )
  linking_recs["descriptionVariants"] = cleaned_descriptions

  placeholder_section = {
    "heading": payload.get("relatedTopicsSection", {}).get("heading")
    or "Explore related topics",
    "links": [dict(link) for link in PLACEHOLDER_RELATED_LINKS],
  }
  payload["relatedTopicsSection"] = placeholder_section

  structured_data = build_structured_data(row, payload)
  if structured_data:
    payload["structuredData"] = structured_data
  else:
    payload.pop("structuredData", None)
  return payload


def serialize_pages(pages: List[Dict[str, Any]]) -> str:
  body = ",\n".join(
    "  " + json.dumps(page, ensure_ascii=False, indent=2)
    .replace("\n", "\n  ")
    .replace("\\u2019", "’")
    for page in pages
  )
  return f"{OUTPUT_HEADER}{body}\n{OUTPUT_FOOTER}"


def write_output_file(path: Path, pages: List[Dict[str, Any]]) -> None:
  """Atomically write the generated pages to ``path``."""

  serialized = serialize_pages(pages)
  path.parent.mkdir(parents=True, exist_ok=True)

  temp_path = path.with_suffix(path.suffix + ".tmp")

  max_retries = 3
  for attempt in range(max_retries):
    try:
      temp_path.write_text(serialized, encoding="utf-8")
      temp_path.replace(path)
      return
    except PermissionError as e:
      if attempt == max_retries - 1:
        # Last attempt failed, try alternative approach
        print(f"Permission denied writing to {path}. Trying alternative method...", file=sys.stderr)
        try:
          # Try writing directly to the file (may leave partial content if interrupted)
          path.write_text(serialized, encoding="utf-8")
          return
        except PermissionError:
          raise PermissionError(
            f"Cannot write to {path}. The file may be open in another application (like your editor). "
            f"Please close the file and try again. Original error: {e}"
          )
      else:
        print(f"Permission denied (attempt {attempt + 1}/{max_retries}). Retrying...", file=sys.stderr)
        time.sleep(0.5)


def load_existing_pages(path: Path) -> List[Dict[str, Any]]:
  if not path.exists():
    return []

  text = path.read_text(encoding="utf-8")
  # Find the opening "[" that starts the array literal assigned to
  # generatedMindMapPages. The first "[" in the file belongs to the
  # ProgrammaticMindMapPage[] type annotation, so we explicitly search for
  # the "[" that follows the equals sign in the export.
  array_start_match = re.search(
    r"generatedMindMapPages[^=]*=\s*\[",
    text,
    flags=re.MULTILINE,
  )
  if not array_start_match:
    print(
      f"Warning: Could not locate generatedMindMapPages array in {path}. Ignoring its contents.",
      file=sys.stderr,
    )
    return []

  start = array_start_match.end() - 1

  # The array literal is terminated by the final "]" in the "\n];" footer.
  end_index = text.rfind("];")
  if end_index == -1 or end_index < start:
    print(
      f"Warning: Could not locate JSON array terminator in existing file {path}. Ignoring its contents.",
      file=sys.stderr,
    )
    return []

  try:
    parsed = json.loads(text[start : end_index + 1])
  except json.JSONDecodeError as exc:
    print(
      f"Warning: Failed to parse existing pages from {path}: {exc}. Ignoring its contents.",
      file=sys.stderr,
    )
    return []

  if not isinstance(parsed, list):
    print(
      f"Warning: Expected a list of pages in {path}, found {type(parsed).__name__}. Ignoring its contents.",
      file=sys.stderr,
    )
    return []

  pages: List[Dict[str, Any]] = []
  for item in parsed:
    if isinstance(item, dict):
      pages.append(item)
  return pages


def main(argv: Iterable[str] | None = None) -> int:
  args = parse_args(argv)
  input_path = Path(args.input)
  output_path = Path(args.output)

  rows = read_csv_rows(input_path)
  if not rows:
    print("No rows found in input CSV", file=sys.stderr)
    return 1

  client = OpenAI(
    api_key=args.api_key or "GEMINI_API_KEY",  # Use placeholder as per custom instructions
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
  )
  existing_pages = load_existing_pages(output_path)
  generated_pages: List[Dict[str, Any]] = list(existing_pages)
  slug_to_index: Dict[str, int] = {}
  for idx, page in enumerate(existing_pages):
    slug = page.get("slug") if isinstance(page, dict) else None
    if isinstance(slug, str):
      slug_to_index[slug] = idx

  regenerated_count = 0
  writes_performed = 0
  failed_rows: List[str] = []

  rows_to_generate: List[CsvRow] = []
  for row in rows:
    if row.slug in slug_to_index and not args.rerun_existing:
      print(f"Skipping slug '{row.slug}' (already present in {output_path})")
      continue
    rows_to_generate.append(row)

  max_api_calls = args.max_api_calls
  if max_api_calls is not None:
    if max_api_calls <= 0:
      rows_to_generate = []
    else:
      rows_to_generate = rows_to_generate[: max_api_calls]

  if rows_to_generate:
    with ThreadPoolExecutor(max_workers=max(1, args.concurrency)) as executor:
      future_to_row = {
        executor.submit(
          call_model_with_retries,
          client,
          args.model,
          args.temperature,
          row.prompt_payload(),
          args.reasoning_effort,
        ): row
        for row in rows_to_generate
      }

      for future in as_completed(future_to_row):
        row = future_to_row[future]
        try:
          payload = future.result()
        except Exception as exc:  # noqa: BLE001
          failed_rows.append(row.slug)
          print(
            f"Error generating slug '{row.slug}': {exc}",
            file=sys.stderr,
          )
          continue

        page = normalise_page(row, payload)

        if row.slug in slug_to_index:
          generated_pages[slug_to_index[row.slug]] = page
        else:
          slug_to_index[row.slug] = len(generated_pages)
          generated_pages.append(page)

        regenerated_count += 1

        write_output_file(output_path, generated_pages)
        writes_performed += 1

  if not output_path.exists() or not writes_performed:
    write_output_file(output_path, generated_pages)

  if failed_rows:
    print(
      "The following slugs failed to generate: " + ", ".join(sorted(failed_rows)),
      file=sys.stderr,
    )
    return 1

  print(
    f"Wrote {len(generated_pages)} programmatic mind map pages to {output_path}"
    + (f" (regenerated {regenerated_count} rows)" if regenerated_count else "")
  )
  return 0


if __name__ == "__main__":
  raise SystemExit(main())
