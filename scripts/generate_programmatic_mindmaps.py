#!/usr/bin/env python3
"""Generate Programmatic Mind Map landing pages from a CSV via the OpenAI SDK.

This script reads a CSV of page definitions, prompts a Gemini-capable OpenAI model
for rich SEO copy, and emits TypeScript objects that satisfy the
``ProgrammaticMindMapPage`` schema. Run it locally before committing the resulting
TypeScript so that every generated page can be statically rendered by Next.js.

Example usage:

python scripts/generate_programmatic_mindmaps.py --input data/mindmap_pages.csv --output lib/programmatic/generated/mindMapPages.ts --model gemini-flash-lite-latest --max-api-calls 500 --concurrency 10

The input CSV must include, at minimum, the columns ``slug`` and
``target_keyword``. Every other column becomes context that is injected into the
LLM prompt so you can steer copy for different audiences, intents, CTAs, etc.
"""

from __future__ import annotations

import argparse
import copy
import csv
import json
import random
import re
import sys
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import deque
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Mapping

from openai import APIError, OpenAI, RateLimitError

DEFAULT_MODEL = "gemini-flash-lite-latest"
DEFAULT_TEMPERATURE = 1.0
DEFAULT_CONCURRENCY = 15
OUTPUT_HEADER = """// @ts-nocheck
// This file is autogenerated by scripts/generate_programmatic_mindmaps.py
// Do not edit by hand — update the CSV and rerun the generator.

import type { ProgrammaticMindMapPage } from '@/lib/programmatic/mindMapPageSchema';

export const generatedMindMapPages: ProgrammaticMindMapPage[] = [
"""
OUTPUT_FOOTER = "];\n"

PLACEHOLDER_RELATED_LINKS = [
  {"label": "/", "href": "/"},
  {"label": "/mind-maps", "href": "/mind-maps"},
]

PROMPT_TEMPLATE = """
You will generate a complete landing page JSON for CogniGuide’s AI mind map generator app, suitable for static rendering.
Info on app: Users upload PDFs, DOCX, PPTX, text, or prompt the AI to build interactive mind maps. CogniGuide restructures the content into expandable branches, exports (PNG/PDF), collaboration via share links. Do not claim unrelated features (you can not edit mind maps for now, do not mention this).

You will receive structured CSV data for one landing page. Follow these requirements precisely:

CRITICAL WRITING RULES (people-first + E-E-A-T):
1)  **Write "People-First" Content:** Prioritize helpful, reliable, original explanations that satisfy the searcher’s mind map intent. Blend storytelling with concrete workflow examples so each page feels handcrafted.
2)  **Demonstrate E-E-A-T:**
    * **Tone:** Confident, instructive, and encouraging—speak as a product marketer who deeply understands visual thinking.
    * **Experience:** Reference specific mind map workflows (study outlines, SOP mapping, meeting debriefs, research synthesis) instead of generic language.
    * **Expertise:** Naturally weave semantic phrases like “diagram complex systems,” “concept mapping,” “hierarchical structure,” “brainstorm visibility,” “visual knowledge base.”
    * **Evidence & Claims:** Use qualitative proof (“teams unblock workshops faster”) instead of unsupported numbers/studies.

PSYCHOLOGY-DRIVEN CTR GUIDANCE FOR METADATA:
- Curiosity gap: Promise a transformation (“turn chaotic research into visual clarity”) while staying truthful.
- Loss aversion: Highlight pain avoided by using the app (Time saving).
- Emotional resonance: Mirror the target audience’s stress in the meta description.

3) Title requirements:
- <=60 characters.
- Include the target keyword or its natural variant in the first half.
- Communicate the exact benefit (“AI mind map maker for nursing students,” etc.).

4) Meta description (130 chars max):
- Mention the target keyword once.
- Describe the real workflow (upload document/type → AI generates mind maps you can export).
- Include two benefits max (e.g., faster clarity, easier collaboration).
- End with a soft CTA (“Start mapping free”).

5) On-page copy:
- Hero heading/subheading must clarify the use-case and point to visual outcomes (clarity, alignment, studying).
- Features section: 3 benefits that speak to inputs (files, prompts), outputs (structure, exports), and downstream actions (teaching, decision-making).
- How it works: Exactly 3 steps tuned to the CSV row’s context. Emphasize visual transformation, editing, and next action (export/share/flashcards).
- SEO section: Body paragraphs + list should reference adjacent keywords (concept maps, idea maps, brainstorming, curriculum planning, etc.).
- FAQ: 4 topic-specific Q&As. Show empathy for blockers (format support, collaboration, exporting, reliability).

6) Embedded mind map preview:
- Provide a Markdown string in Markmap syntax that mirrors what a user searching the target keyword expects to visualize.
- Keep hierarchy 3 levels deep max; no lorem ipsum. Emphasize subtopics and supporting bullets.
- Do not return separate title/description fields; the top-level branch should read like the map title.

7) Structured data:
- Provide breadcrumb + FAQ JSON-LD if omitted; your JSON output may include it directly, but it must follow schema.org formats.

Return JSON with this shape (comments describe expectations). The slug, path, and canonical URL are injected from the CSV slug by the generator—do not include or invent them:

```json
{
  "metadata": {
    "title": string,            // ≤60 characters, includes target keyword
    "description": string,      // 120 characters
    "keywords": string[]        // 5–10 semantic variants (for internal use; not meta keywords).
  },
  "hero": {
    "heading": string,        // contains the target keyword or closest variant
    "subheading": string,
    "primaryCta": { "type": "modal", "label": "Generate mind map" }
  },
  "featuresSection": {
    "heading": string,
    "subheading": string,
    "features": [{ "title": string, "description": string }, ...] // 3–4 cards
  },
  "howItWorksSection": {
    "heading": string,
    "subheading": string,
    "steps": [{ "title": string, "description": string }, ...], // exactly 3
    "cta": { "type": "modal", "label": "Generate mind map" }
  },
  "seoSection": {
    "heading": string,
    "body": [
      { "type": "paragraph", "html": string }, // first sentence starts with the keyword or close variant
      { "type": "list", "items": [string, ...] },
      { "type": "paragraph", "html": string }
    ]
  },
  "faqSection": {
    "heading": string,
    "subheading": string,
    "items": [{ "question": string, "answer": string }, ...], // 4 items
    "cta": { "type": "modal", "label": string }
  },
  "linkingRecommendations": {
    "anchorText": string, // 3–5 words, must include the keyword variant
    "descriptionVariants": [string, string]
  },
  "embeddedMindMap": {
    "markdown": string // Valid Markmap markdown preview tailored to the keyword; first parent node conveys the title
  },
  "relatedTopicsSection": {
    "heading": string,
    "links": [{ "label": string, "href": string, "description": string }, ...]
  }
}
```

QUALITY GATES:
- Never return placeholders. Every string must be production-ready.
- Avoid duplication; each page must feel unique to its keyword/audience.
- Keep copy grounded in real app functionality (upload, prompt, edit, export, convert to flashcards).
- Embedded mind map markdown must be safe for HTML (no raw script tags) and use bullet hierarchy only.
- linkingRecommendations.descriptionVariants must each mention the target keyword or a natural derivative.
"""

REASONING_EFFORT_BUDGETS = {
  "low": 512,
  "medium": 1024,
  "high": 8192,
}


SCRIPT_DIR = Path(__file__).resolve().parent
REPO_ROOT = SCRIPT_DIR.parent
TAXONOMY_PATH = REPO_ROOT / "data" / "mindmap_taxonomy.json"

TaxonomyEntry = Dict[str, str]
_TAXONOMY_CACHE: Dict[str, TaxonomyEntry] | None = None


def slugify(value: str) -> str:
  normalized = value.lower().replace("&", "and")
  normalized = re.sub(r"[^a-z0-9]+", "-", normalized)
  return normalized.strip("-")


def load_taxonomy_map() -> Dict[str, TaxonomyEntry]:
  global _TAXONOMY_CACHE
  if _TAXONOMY_CACHE is not None:
    return _TAXONOMY_CACHE

  try:
    raw = json.loads(TAXONOMY_PATH.read_text(encoding="utf-8"))
  except FileNotFoundError:
    _TAXONOMY_CACHE = {}
    return _TAXONOMY_CACHE

  mapping: Dict[str, TaxonomyEntry] = {}
  if isinstance(raw, dict):
    for hub_name, subhub_map in raw.items():
      if not isinstance(subhub_map, dict):
        continue
      hub_slug = slugify(str(hub_name))
      for subhub_name, slugs in subhub_map.items():
        if not isinstance(slugs, list):
          continue
        subhub_slug = slugify(str(subhub_name))
        for slug in slugs:
          if not isinstance(slug, str):
            continue
          mapping.setdefault(
            slug,
            {
              "hub_name": str(hub_name),
              "hub_slug": hub_slug,
              "subhub_name": str(subhub_name),
              "subhub_slug": subhub_slug,
            },
          )

  _TAXONOMY_CACHE = mapping
  return _TAXONOMY_CACHE


def build_breadcrumb_segments(row: CsvRow, breadcrumb_name: str) -> List[Dict[str, str]]:
  base_url = row.base_url.rstrip("/")
  segments: List[Dict[str, str]] = [
    {"name": "Home", "item": f"{base_url}/"},
    {"name": "Mind Maps", "item": f"{base_url}/mind-maps/"},
  ]

  taxonomy = load_taxonomy_map()
  entry = taxonomy.get(row.slug)
  if isinstance(entry, dict):
    hub_name = entry.get("hub_name")
    hub_slug = entry.get("hub_slug")
    if hub_name and hub_slug:
      segments.append(
        {
          "name": hub_name,
          "item": f"{base_url}/mind-maps/{hub_slug}/",
        }
      )

    subhub_name = entry.get("subhub_name")
    subhub_slug = entry.get("subhub_slug")
    if subhub_name and subhub_slug and hub_slug:
      segments.append(
        {
          "name": subhub_name,
          "item": f"{base_url}/mind-maps/{hub_slug}/{subhub_slug}/",
        }
      )

  breadcrumb_label = breadcrumb_name.strip() or row.slug
  segments.append({"name": breadcrumb_label})
  return segments


def parse_args(argv: Iterable[str] | None = None) -> argparse.Namespace:
  parser = argparse.ArgumentParser(description="Generate programmatic mind map pages")
  parser.add_argument("--input", required=True, help="Path to the CSV containing page definitions")
  parser.add_argument(
    "--output",
    default="lib/programmatic/generated/mindMapPages.ts",
    help="Destination TypeScript file to overwrite",
  )
  parser.add_argument(
    "--model",
    default=DEFAULT_MODEL,
    help="OpenAI model name (Gemini-compatible models supported via the OpenAI SDK)",
  )
  parser.add_argument(
    "--temperature",
    type=float,
    default=DEFAULT_TEMPERATURE,
    help="Sampling temperature for creative variation",
  )
  parser.add_argument(
    "--max-api-calls",
    type=int,
    default=500,
    help="Optional maximum number of Gemini API calls to make (for testing)",
  )
  parser.add_argument(
    "--concurrency",
    type=int,
    default=DEFAULT_CONCURRENCY,
    help="Maximum number of concurrent Gemini API requests to make",
  )
  parser.add_argument(
    "--max-api-calls-per-minute",
    type=int,
    default=10,
    help="Throttle Gemini API usage to this many calls per rolling minute (0 to disable).",
  )
  parser.add_argument(
    "--rerun-existing",
    action="store_true",
    help="Regenerate rows whose slugs already exist in the output file",
  )
  parser.add_argument(
    "--api-key",
    default=None,
    help="Explicit Gemini API key. Falls back to the GEMINI_API_KEY env var if omitted.",
  )
  parser.add_argument(
    "--reasoning-effort",
    choices=["low", "medium", "high", "none"],
    default="low",
    help="Reasoning effort for Gemini 2.5 models: low (1,024 tokens), medium (8,192 tokens), high (24,576 tokens), or none (disable thinking).",
  )
  return parser.parse_args(argv)


@dataclass
class CsvRow:
  slug: str
  data: Mapping[str, str]

  @property
  def path(self) -> str:
    return self.data.get("path") or f"/mind-maps/{self.slug}"

  @property
  def base_url(self) -> str:
    return self.data.get("base_url") or "https://www.cogniguide.app"

  def prompt_payload(self) -> str:
    payload = {
      "slug": self.slug,
      "base_url": self.base_url,
      "context": {k: v for k, v in self.data.items() if k not in {"slug", "path", "base_url"}},
    }
    return json.dumps(payload, ensure_ascii=False, indent=2)


def read_csv_rows(path: Path) -> List[CsvRow]:
  with path.open(newline="", encoding="utf-8") as handle:
    reader = csv.DictReader(handle)
    if "slug" not in reader.fieldnames:
      raise ValueError("Input CSV must include a 'slug' column")
    if "target_keyword" not in reader.fieldnames:
      raise ValueError("Input CSV must include a 'target_keyword' column")

    rows: List[CsvRow] = []
    for idx, raw in enumerate(reader):
      slug = (raw.get("slug") or "").strip()
      if not slug:
        raise ValueError(f"Row {idx + 2} is missing a slug")
      if not (raw.get("target_keyword") or "").strip():
        raise ValueError(f"Row {idx + 2} is missing a target_keyword")
      rows.append(CsvRow(slug=slug, data={k: str(v or '').strip() for k, v in raw.items()}))
    return rows


def _inject_thinking_budget(
  request_params: Dict[str, Any],
  reasoning_effort: str,
) -> Dict[str, Any]:
  """Fallback for SDKs that do not expose reasoning_effort."""

  budget = REASONING_EFFORT_BUDGETS.get(reasoning_effort)
  if not budget:
    return request_params

  extra_body = copy.deepcopy(request_params.get("extra_body") or {})
  nested_extra = extra_body.setdefault("extra_body", {})
  google_section = nested_extra.setdefault("google", {})
  thinking_config = google_section.setdefault("thinking_config", {})
  thinking_config.setdefault("thinking_budget", budget)
  google_section["thinking_config"] = thinking_config
  request_params["extra_body"] = extra_body
  return request_params


class RateLimiter:
  """Thread-safe rolling window limiter for API calls."""

  def __init__(self, max_calls_per_minute: int | None):
    self.max_calls_per_minute = max_calls_per_minute or 0
    self.lock = threading.Lock()
    self.calls: deque[float] = deque()

  def wait_for_slot(self) -> None:
    if self.max_calls_per_minute <= 0:
      return

    while True:
      now = time.time()
      with self.lock:
        # Drop calls older than 60 seconds
        while self.calls and now - self.calls[0] >= 60:
          self.calls.popleft()

        if len(self.calls) < self.max_calls_per_minute:
          self.calls.append(now)
          return

        oldest_call = self.calls[0]
        sleep_for = max(0.01, 60 - (now - oldest_call))
      time.sleep(sleep_for)


def call_model(
  client: OpenAI,
  model: str,
  temperature: float,
  payload: str,
  reasoning_effort: str | None = None,
) -> Dict[str, Any]:
  # Prepare the request parameters
  request_params = {
    "model": model,
    "temperature": temperature,
    "messages": [
      {
        "role": "system",
        "content": PROMPT_TEMPLATE,
      },
      {
        "role": "user",
        "content": f"CSV row JSON:\n{payload}",
      },
    ],
    "response_format": {
      "type": "json_schema",
      "json_schema": {
        "name": "programmatic_mindmap_page",
        "schema": {
          "type": "object",
          "properties": {
            "metadata": {"type": "object"},
            "hero": {"type": "object"},
            "featuresSection": {"type": "object"},
            "howItWorksSection": {"type": "object"},
            "seoSection": {"type": "object"},
            "faqSection": {"type": "object"},
            "relatedTopicsSection": {"type": "object"},
            "linkingRecommendations": {
              "type": "object",
              "properties": {
                "anchorText": {"type": "string"},
                "descriptionVariants": {
                  "type": "array",
                  "items": {"type": "string"},
                  "minItems": 2,
                  "maxItems": 2,
                },
              },
              "required": ["anchorText", "descriptionVariants"],
              "additionalProperties": False,
            },
            "embeddedMindMap": {
              "type": "object",
              "properties": {
                "markdown": {"type": "string"},
              },
              "required": ["markdown"],
              "additionalProperties": False,
            },
          },
          "required": [
            "metadata",
            "hero",
            "featuresSection",
            "howItWorksSection",
            "seoSection",
            "faqSection",
            "linkingRecommendations",
            "embeddedMindMap",
          ],
          "additionalProperties": True,
        },
      },
    },
  }

  # Add reasoning_effort if provided and not "none"
  use_reasoning = reasoning_effort and reasoning_effort != "none"
  if use_reasoning:
    request_params["reasoning_effort"] = reasoning_effort

  try:
    response = client.chat.completions.create(**request_params)
  except TypeError as exc:
    if not use_reasoning or "reasoning_effort" not in str(exc):
      raise

    # Some older OpenAI SDK builds reject reasoning_effort; fall back to thinking budget.
    assert reasoning_effort is not None
    fallback_params = copy.deepcopy(request_params)
    fallback_params.pop("reasoning_effort", None)
    fallback_params = _inject_thinking_budget(fallback_params, reasoning_effort)
    response = client.chat.completions.create(**fallback_params)

  if not response.choices or not response.choices[0].message.content:
    raise RuntimeError("Model returned an empty response")

  return json.loads(response.choices[0].message.content)


def call_model_with_retries(
  client: OpenAI,
  model: str,
  temperature: float,
  payload: str,
  reasoning_effort: str | None = None,
  *,
  max_attempts: int = 6,
  initial_delay: float = 4.0,
  rate_limiter: RateLimiter | None = None,
) -> Dict[str, Any]:
  delay = initial_delay
  for attempt in range(1, max_attempts + 1):
    try:
      if rate_limiter:
        rate_limiter.wait_for_slot()
      return call_model(client, model, temperature, payload, reasoning_effort)
    except RateLimitError:
      if attempt == max_attempts:
        raise
      sleep_for = delay * (1 + random.random())
      print(
        f"Rate limit encountered (attempt {attempt}/{max_attempts}). "
        f"Retrying in {sleep_for:.2f} seconds...",
        file=sys.stderr,
      )
      time.sleep(sleep_for)
      delay *= 2
    except APIError as exc:
      status = getattr(exc, "status", None)
      if status == 429 and attempt < max_attempts:
        sleep_for = delay * (1 + random.random())
        print(
          f"Gemini API returned status 429 (attempt {attempt}/{max_attempts}). "
          f"Retrying in {sleep_for:.2f} seconds...",
          file=sys.stderr,
        )
        time.sleep(sleep_for)
        delay *= 2
        continue
      raise


def build_structured_data(row: CsvRow, payload: Dict[str, Any]) -> Dict[str, Any] | None:
  """Return structured data for ``payload`` when the model omits it."""

  metadata = payload.get("metadata") if isinstance(payload, dict) else {}
  if not isinstance(metadata, dict):
    metadata = {}

  path = payload.get("path") if isinstance(payload, dict) else None
  if not isinstance(path, str) or not path:
    path = row.path

  canonical = metadata.get("canonical")
  if not isinstance(canonical, str) or not canonical:
    canonical = f"{row.base_url.rstrip('/')}{path}"

  hero = payload.get("hero") if isinstance(payload, dict) else {}
  hero_heading = None
  if isinstance(hero, dict):
    heading = hero.get("heading")
    if isinstance(heading, str) and heading.strip():
      hero_heading = heading.strip()

  breadcrumb_name = hero_heading or row.data.get("target_keyword") or row.slug

  faq_section = payload.get("faqSection") if isinstance(payload, dict) else {}
  faq_items = []
  if isinstance(faq_section, dict):
    raw_items = faq_section.get("items")
    if isinstance(raw_items, list):
      for item in raw_items:
        if not isinstance(item, dict):
          continue
        question = item.get("question")
        answer = item.get("answer")
        if isinstance(question, str) and isinstance(answer, str) and question.strip() and answer.strip():
          faq_items.append(
            {
              "@type": "Question",
              "name": question.strip(),
              "acceptedAnswer": {"@type": "Answer", "text": answer.strip()},
            }
          )

  breadcrumb_segments = build_breadcrumb_segments(row, breadcrumb_name)
  item_list: List[Dict[str, Any]] = []
  for idx, segment in enumerate(breadcrumb_segments, start=1):
    item: Dict[str, Any] = {
      "@type": "ListItem",
      "position": idx,
      "name": segment.get("name") or "",
    }
    href = segment.get("item")
    if href:
      item["item"] = href
    item_list.append(item)

  breadcrumb_graph = {
    "@type": "BreadcrumbList",
    "itemListElement": item_list,
  }

  graph = [breadcrumb_graph]

  if faq_items:
    graph.append(
      {
        "@type": "FAQPage",
        "@id": f"{canonical}#faq",
        "url": canonical,
        "inLanguage": "en",
        "name": metadata.get("title") or breadcrumb_name,
        "mainEntity": faq_items,
      }
    )

  if not graph:
    return None

  return {"@context": "https://schema.org", "@graph": graph}


def normalize_embedded_mindmap_markdown(markdown: str, fallback_title: str) -> str:
  """
  Make the embedded mind map markdown resilient to slight model format drift.

  The renderer only understands Markmap-style headings/lists. Models occasionally return
  outline-style text with indentation but no list markers or missing frontmatter fences.
  This normalises the string by:
  - Stripping empty lines
  - Converting a leading "title: ..." line into a heading when frontmatter is missing
  - Injecting a root heading when none exists
  - Prefixing non-heading lines with "- " (preserving indentation) so they become list items
  """

  if not isinstance(markdown, str):
    raise ValueError("embeddedMindMap.markdown must be a string")

  lines = [line.rstrip() for line in markdown.splitlines()]
  while lines and not lines[0].strip():
    lines.pop(0)
  while lines and not lines[-1].strip():
    lines.pop()

  frontmatter: List[str] = []
  body_lines: List[str] = list(lines)

  # Preserve valid frontmatter blocks
  if body_lines and body_lines[0].strip() == "---":
    try:
      closing_index = next(
        idx for idx, line in enumerate(body_lines[1:], start=1) if line.strip() == "---"
      )
      frontmatter = body_lines[: closing_index + 1]
      body_lines = body_lines[closing_index + 1 :]
    except StopIteration:
      # Treat as no frontmatter if the closing fence is missing
      pass

  title_override = None
  if not frontmatter and body_lines and body_lines[0].lower().startswith("title:"):
    title_override = body_lines[0].split(":", 1)[1].strip() or None
    body_lines = body_lines[1:]
    while body_lines and body_lines[0].strip() == "---":
      body_lines = body_lines[1:]

  body_lines = [line for line in body_lines if line.strip()]
  has_heading = any(re.match(r"^\s*#\s", line) for line in body_lines)
  effective_title = title_override or fallback_title or "Mind Map"

  normalized_body: List[str] = []
  if not has_heading:
    normalized_body.append(f"# {effective_title}")

  token_pattern = re.compile(r"^\s*(?:[-*+]|\d+\.)\s|^\s*#\s")
  for line in body_lines:
    if not line.strip():
      continue
    if token_pattern.match(line):
      normalized_body.append(line)
      continue

    stripped = line.lstrip()
    indent = len(line) - len(stripped)
    normalized_body.append(f"{' ' * indent}- {stripped}")

  # Safety: ensure we always return something meaningful
  if not normalized_body:
    normalized_body = [f"# {effective_title}", "- Key points"]

  return "\n".join(frontmatter + normalized_body)


def normalise_page(row: CsvRow, payload: Dict[str, Any]) -> Dict[str, Any]:
  metadata = payload.get("metadata")
  if not isinstance(metadata, dict):
    raise ValueError(f"Model response for slug '{row.slug}' is missing metadata")

  metadata["canonical"] = f"{row.base_url.rstrip('/')}{row.path}"

  payload["path"] = row.path
  payload["slug"] = row.slug

  linking_recs = payload.get("linkingRecommendations")
  if not isinstance(linking_recs, dict):
    raise ValueError(
      f"Model response for slug '{row.slug}' is missing linkingRecommendations"
    )

  anchor_text = linking_recs.get("anchorText")
  if not isinstance(anchor_text, str) or not anchor_text.strip():
    raise ValueError(
      f"linkingRecommendations.anchorText must be a non-empty string for slug '{row.slug}'"
    )
  linking_recs["anchorText"] = anchor_text.strip()

  description_variants = linking_recs.get("descriptionVariants")
  if not isinstance(description_variants, list) or len(description_variants) < 2:
    raise ValueError(
      f"linkingRecommendations.descriptionVariants must include two entries for slug '{row.slug}'"
    )
  cleaned_descriptions = [
    str(item).strip() for item in description_variants if isinstance(item, str)
  ][:2]
  if len(cleaned_descriptions) < 2:
    raise ValueError(
      f"linkingRecommendations.descriptionVariants contained invalid entries for slug '{row.slug}'"
    )
  linking_recs["descriptionVariants"] = cleaned_descriptions

  embedded_mindmap = payload.get("embeddedMindMap")
  if not isinstance(embedded_mindmap, dict):
    raise ValueError(f"Model response for slug '{row.slug}' is missing embeddedMindMap")
  markdown = embedded_mindmap.get("markdown")
  if not isinstance(markdown, str) or not markdown.strip():
    raise ValueError(
      f"embeddedMindMap.markdown must be a non-empty string for slug '{row.slug}'"
    )

  hero = payload.get("hero") if isinstance(payload.get("hero"), dict) else {}
  hero_heading = hero.get("heading") if isinstance(hero, dict) else None
  metadata_title = metadata.get("title") if isinstance(metadata, dict) else None
  target_keyword = row.data.get("target_keyword")
  fallback_title = (
    (hero_heading or "").strip()
    or (metadata_title or "").strip()
    or (target_keyword or "").strip()
    or row.slug.replace("-", " ").strip()
  )
  embedded_mindmap["markdown"] = normalize_embedded_mindmap_markdown(
    markdown, fallback_title
  )

  placeholder_section = {
    "heading": payload.get("relatedTopicsSection", {}).get("heading")
    or "Explore related topics",
    "links": [dict(link) for link in PLACEHOLDER_RELATED_LINKS],
  }
  payload["relatedTopicsSection"] = placeholder_section

  structured_data = build_structured_data(row, payload)
  if structured_data:
    payload["structuredData"] = structured_data
  else:
    payload.pop("structuredData", None)
  return payload


def serialize_pages(pages: List[Dict[str, Any]]) -> str:
  body = ",\n".join(
    "  " + json.dumps(page, ensure_ascii=False, indent=2)
    .replace("\n", "\n  ")
    .replace("\\u2019", "’")
    for page in pages
  )
  return f"{OUTPUT_HEADER}{body}\n{OUTPUT_FOOTER}"


def write_output_file(path: Path, pages: List[Dict[str, Any]]) -> None:
  """Atomically write the generated pages to ``path``."""

  serialized = serialize_pages(pages)
  path.parent.mkdir(parents=True, exist_ok=True)

  temp_path = path.with_suffix(path.suffix + ".tmp")

  max_retries = 3
  for attempt in range(max_retries):
    try:
      temp_path.write_text(serialized, encoding="utf-8")
      temp_path.replace(path)
      return
    except PermissionError as e:
      if attempt == max_retries - 1:
        # Last attempt failed, try alternative approach
        print(f"Permission denied writing to {path}. Trying alternative method...", file=sys.stderr)
        try:
          # Try writing directly to the file (may leave partial content if interrupted)
          path.write_text(serialized, encoding="utf-8")
          return
        except PermissionError:
          raise PermissionError(
            f"Cannot write to {path}. The file may be open in another application (like your editor). "
            f"Please close the file and try again. Original error: {e}"
          )
      else:
        print(f"Permission denied (attempt {attempt + 1}/{max_retries}). Retrying...", file=sys.stderr)
        time.sleep(0.5)


def load_existing_pages(path: Path) -> List[Dict[str, Any]]:
  if not path.exists():
    return []

  text = path.read_text(encoding="utf-8")
  # Find the opening "[" that starts the array literal assigned to
  # generatedMindMapPages. The first "[" in the file belongs to the
  # ProgrammaticMindMapPage[] type annotation, so we explicitly search for
  # the "[" that follows the equals sign in the export.
  array_start_match = re.search(
    r"generatedMindMapPages[^=]*=\s*\[",
    text,
    flags=re.MULTILINE,
  )
  if not array_start_match:
    print(
      f"Warning: Could not locate generatedMindMapPages array in {path}. Ignoring its contents.",
      file=sys.stderr,
    )
    return []

  start = array_start_match.end() - 1

  # The array literal is terminated by the final "]" in the "\n];" footer.
  end_index = text.rfind("];")
  if end_index == -1 or end_index < start:
    print(
      f"Warning: Could not locate JSON array terminator in existing file {path}. Ignoring its contents.",
      file=sys.stderr,
    )
    return []

  try:
    parsed = json.loads(text[start : end_index + 1])
  except json.JSONDecodeError as exc:
    print(
      f"Warning: Failed to parse existing pages from {path}: {exc}. Ignoring its contents.",
      file=sys.stderr,
    )
    return []

  if not isinstance(parsed, list):
    print(
      f"Warning: Expected a list of pages in {path}, found {type(parsed).__name__}. Ignoring its contents.",
      file=sys.stderr,
    )
    return []

  pages: List[Dict[str, Any]] = []
  for item in parsed:
    if isinstance(item, dict):
      pages.append(item)
  return pages


def main(argv: Iterable[str] | None = None) -> int:
  args = parse_args(argv)
  input_path = Path(args.input)
  output_path = Path(args.output)

  rows = read_csv_rows(input_path)
  if not rows:
    print("No rows found in input CSV", file=sys.stderr)
    return 1

  client = OpenAI(
    api_key=args.api_key or "GEMINI_API_KEY",  # Use placeholder as per custom instructions
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
  )
  existing_pages = load_existing_pages(output_path)
  generated_pages: List[Dict[str, Any]] = list(existing_pages)
  slug_to_index: Dict[str, int] = {}
  for idx, page in enumerate(existing_pages):
    slug = page.get("slug") if isinstance(page, dict) else None
    if isinstance(slug, str):
      slug_to_index[slug] = idx

  regenerated_count = 0
  writes_performed = 0
  failed_rows: List[str] = []

  rows_to_generate: List[CsvRow] = []
  for row in rows:
    if row.slug in slug_to_index and not args.rerun_existing:
      print(f"Skipping slug '{row.slug}' (already present in {output_path})")
      continue
    rows_to_generate.append(row)

  max_api_calls = args.max_api_calls
  if max_api_calls is not None:
    if max_api_calls <= 0:
      rows_to_generate = []
    else:
      rows_to_generate = rows_to_generate[: max_api_calls]

  rate_limiter = RateLimiter(args.max_api_calls_per_minute)

  if rows_to_generate:
    with ThreadPoolExecutor(max_workers=max(1, args.concurrency)) as executor:
      future_to_row = {
        executor.submit(
          call_model_with_retries,
          client,
          args.model,
          args.temperature,
          row.prompt_payload(),
          args.reasoning_effort,
          rate_limiter=rate_limiter,
        ): row
        for row in rows_to_generate
      }

      for future in as_completed(future_to_row):
        row = future_to_row[future]
        try:
          payload = future.result()
        except Exception as exc:  # noqa: BLE001
          failed_rows.append(row.slug)
          print(
            f"Error generating slug '{row.slug}': {exc}",
            file=sys.stderr,
          )
          continue

        page = normalise_page(row, payload)

        if row.slug in slug_to_index:
          generated_pages[slug_to_index[row.slug]] = page
        else:
          slug_to_index[row.slug] = len(generated_pages)
          generated_pages.append(page)

        regenerated_count += 1

        write_output_file(output_path, generated_pages)
        writes_performed += 1

  if not output_path.exists() or not writes_performed:
    write_output_file(output_path, generated_pages)

  if failed_rows:
    print(
      "The following slugs failed to generate: " + ", ".join(sorted(failed_rows)),
      file=sys.stderr,
    )
    return 1

  print(
    f"Wrote {len(generated_pages)} programmatic mind map pages to {output_path}"
    + (f" (regenerated {regenerated_count} rows)" if regenerated_count else "")
  )
  return 0


if __name__ == "__main__":
  raise SystemExit(main())
